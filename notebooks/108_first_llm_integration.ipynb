{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f3a821",
   "metadata": {},
   "source": [
    "# 108 LangGraph: Your First LLM Integration (Simple Bot)\n",
    "\n",
    "**Workshop**: LangGraph 108\n",
    "**Duration**: ~30 minutes\n",
    "**Difficulty**: Intermediate\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Integrate an LLM (Large Language Model) into your LangGraph workflows\n",
    "- Define state structures for handling AI messages\n",
    "- Initialize and invoke Anthropic's Claude models using LangChain\n",
    "- Build a simple AI bot for PAN-OS firewall queries\n",
    "- Understand the difference between a simple bot and a true conversational agent\n",
    "- Discover the limitations of stateless AI interactions (no memory)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Knowledge**: Completed notebooks 101-107 (Type Annotations, Core Concepts, Graph Patterns)\n",
    "- **Setup**: Anthropic API key\n",
    "- **Cost**: Very minimal - Claude Haiku costs pennies per 1000 tokens\n",
    "\n",
    "**Important**: This notebook introduces LLM integration after mastering graph patterns in notebooks 103-107. If you haven't completed those notebooks, please go back and work through the graph pattern foundations first.\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "In this notebook, we'll build a **Simple PAN-OS AI Bot** that can answer questions about firewall operations, upgrades, and configurations. However, it won't remember previous messages in the conversation - and that's intentional! This limitation will motivate the next notebook where we add memory.\n",
    "\n",
    "**Graph Structure:**\n",
    "```\n",
    "START â†’ process_query â†’ END\n",
    "```\n",
    "\n",
    "Simple, but powerful - this is where AI meets LangGraph!\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Setup and Imports](#2-setup-and-imports)\n",
    "3. [Understanding Human Messages](#3-understanding-human-messages)\n",
    "4. [Defining the Agent State](#4-defining-the-agent-state)\n",
    "5. [Initializing the LLM](#5-initializing-the-llm)\n",
    "6. [Building the Agent Graph](#6-building-the-agent-graph)\n",
    "7. [Testing the Simple Bot](#7-testing-the-simple-bot)\n",
    "8. [The Memory Problem](#8-the-memory-problem)\n",
    "9. [Summary](#9-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Welcome to your **first LLM-integrated LangGraph agent**! This is an exciting milestone in your workshop journey.\n",
    "\n",
    "### What We're Building\n",
    "\n",
    "In notebooks 103-107, we built LangGraph workflows without any AI. We learned:\n",
    "- How to define state structures (103-104)\n",
    "- How to create nodes and connect them (103)\n",
    "- Sequential workflows, conditional routing, and loops (105-107)\n",
    "\n",
    "But we haven't actually integrated an **LLM (Large Language Model)** yet. That changes today!\n",
    "\n",
    "### The Simple Bot\n",
    "\n",
    "We're building what I call a **Simple Bot** - a basic LLM wrapper that can:\n",
    "- Accept questions about PAN-OS firewalls\n",
    "- Send those questions to an AI model (Claude)\n",
    "- Return intelligent responses\n",
    "\n",
    "### Why \"Simple\" Bot?\n",
    "\n",
    "This bot has a critical limitation: **it has no memory**. Each question is treated independently, with no context from previous messages. \n",
    "\n",
    "Imagine asking:\n",
    "1. \"My firewall is fw-prod-01\"\n",
    "2. \"What's the hostname of my firewall?\"\n",
    "\n",
    "The bot won't remember you just told it! This limitation is intentional - it'll motivate the next notebook where we add conversation memory.\n",
    "\n",
    "### Real-World Use Case\n",
    "\n",
    "Even without memory, a simple bot is useful for:\n",
    "- One-off firewall configuration queries\n",
    "- Upgrade path recommendations\n",
    "- Quick command syntax lookups\n",
    "- Policy best practices\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Setup and Imports\n",
    "\n",
    "### Important: LangChain + LangGraph Together\n",
    "\n",
    "You might be wondering: \"Wait, I thought we were learning LangGraph, why are we importing LangChain libraries?\"\n",
    "\n",
    "Great question! Here's the key insight:\n",
    "\n",
    "**LangGraph is built on top of LangChain**. LangChain provides sophisticated, robust libraries for working with LLMs, and LangGraph leverages these strengths. Think of it as:\n",
    "- **LangChain**: Tools for working with LLMs (models, messages, prompts)\n",
    "- **LangGraph**: Framework for orchestrating LLMs into multi-step workflows\n",
    "\n",
    "They work together beautifully!\n",
    "\n",
    "### About API Keys\n",
    "\n",
    "We need an API key because we're making calls to an external LLM service (Anthropic). If you were using a local LLM (like through Ollama), you wouldn't need an API key.\n",
    "\n",
    "We'll store the API key in a `.env` file for security - never hardcode API keys in your notebooks!\n",
    "\n",
    "### Cost Note\n",
    "\n",
    "Don't worry about cost! Claude Haiku is extremely cheap:\n",
    "- Input: ~$0.25 per 1 million tokens\n",
    "- Output: ~$1.25 per 1 million tokens\n",
    "\n",
    "For this workshop, you'll spend pennies at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:50.459398Z",
     "iopub.status.busy": "2025-10-22T09:53:50.459162Z",
     "iopub.status.idle": "2025-10-22T09:53:50.720315Z",
     "shell.execute_reply": "2025-10-22T09:53:50.719912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core typing imports\n",
    "from typing import TypedDict, List\n",
    "\n",
    "# LangChain message types - THIS IS NEW!\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain LLM integration - THIS IS NEW!\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# LangGraph core (familiar from notebooks 103-107)\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Environment variable loading for API keys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(\"\\nWhat's new in this notebook:\")\n",
    "print(\"  - HumanMessage: Represents messages from users to the AI\")\n",
    "print(\"  - AIMessage: Represents responses from the AI to users\")\n",
    "print(\"  - ChatAnthropic: Interface to Anthropic's Claude models\")\n",
    "print(\"  - load_dotenv: Securely loads API keys from .env file\")\n",
    "print(\"\\nðŸ’¡ LangChain + LangGraph work together!\")\n",
    "print(\"   LangChain: LLM tools (models, messages)\")\n",
    "print(\"   LangGraph: Workflow orchestration (graphs, nodes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-setup",
   "metadata": {},
   "source": [
    "### 2.1 Load Environment Variables\n",
    "\n",
    "Before we can use Anthropic's API, we need to load our API key from the `.env` file.\n",
    "\n",
    "**Creating a .env file:**\n",
    "\n",
    "If you don't have one yet, create a file named `.env` in your project root:\n",
    "\n",
    "```bash\n",
    "ANTHROPIC_API_KEY=sk-ant-your-api-key-here\n",
    "```\n",
    "\n",
    "**Security Note:** Never commit `.env` files to git! Add `.env` to your `.gitignore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-env",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:50.721782Z",
     "iopub.status.busy": "2025-10-22T09:53:50.721670Z",
     "iopub.status.idle": "2025-10-22T09:53:50.723932Z",
     "shell.execute_reply": "2025-10-22T09:53:50.723566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment variables loaded!\")\n",
    "print(\"\\nðŸ’¡ The ANTHROPIC_API_KEY is now available for use\")\n",
    "print(\"   (but we won't print it for security reasons!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-message-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding Human Messages\n",
    "\n",
    "### What is a HumanMessage?\n",
    "\n",
    "A `HumanMessage` is a structured way to represent a message from a human (you!) to an AI model. It's part of LangChain's message system.\n",
    "\n",
    "### Why Not Just Use Strings?\n",
    "\n",
    "You might think: \"Why not just pass strings to the LLM?\" \n",
    "\n",
    "LangChain uses typed message objects because:\n",
    "1. **Type Safety**: Clear distinction between human messages, AI messages, system messages\n",
    "2. **Metadata**: Messages can carry additional context (timestamps, IDs, etc.)\n",
    "3. **Conversation History**: Easy to track who said what in multi-turn conversations\n",
    "4. **Consistency**: Works across different LLM providers\n",
    "\n",
    "### Message Types in LangChain\n",
    "\n",
    "There are several message types:\n",
    "- **HumanMessage**: From the user to the AI\n",
    "- **AIMessage**: From the AI to the user\n",
    "- **SystemMessage**: Instructions for the AI (like \"You are a PAN-OS expert\")\n",
    "- **ToolMessage**: Results from tool/function calls\n",
    "\n",
    "For now, we'll focus on `HumanMessage`.\n",
    "\n",
    "Let's see how to create one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-message-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:50.725063Z",
     "iopub.status.busy": "2025-10-22T09:53:50.724962Z",
     "iopub.status.idle": "2025-10-22T09:53:50.727060Z",
     "shell.execute_reply": "2025-10-22T09:53:50.726699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a simple HumanMessage\n",
    "message = HumanMessage(content=\"What is the recommended upgrade path from PAN-OS 10.1 to 10.2?\")\n",
    "\n",
    "print(\"HumanMessage created:\")\n",
    "print(f\"  Type: {type(message).__name__}\")\n",
    "print(f\"  Content: {message.content}\")\n",
    "print(f\"\\nFull message object:\")\n",
    "print(message)\n",
    "\n",
    "print(\"\\nðŸ’¡ HumanMessage wraps the content with metadata!\")\n",
    "print(\"   This tells the LLM: 'This message is from a human'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5giqqhu4od",
   "metadata": {},
   "source": [
    "### 3.1 Preview: SystemMessage (Coming in Notebook 109)\n",
    "\n",
    "We've seen **HumanMessage** (user â†’ AI) and **AIMessage** (AI â†’ user). There's one more important message type: **SystemMessage**.\n",
    "\n",
    "**What is SystemMessage?**\n",
    "\n",
    "SystemMessage provides **instructions and context** to the AI before the conversation starts. Think of it as setting the AI's role, expertise, and personality.\n",
    "\n",
    "**Example SystemMessage:**\n",
    "```python\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_msg = SystemMessage(content=\"\"\"You are an expert PAN-OS network engineer \n",
    "specializing in Strata Cloud Manager automation. You provide clear, accurate \n",
    "technical guidance on firewall configuration, pan-scm-sdk usage, and security \n",
    "best practices. Always include working code examples when relevant.\"\"\")\n",
    "```\n",
    "\n",
    "**Why use SystemMessage?**\n",
    "- **Set expertise**: Make the bot a PAN-OS expert, not generic assistant\n",
    "- **Define personality**: Concise technical vs detailed explanatory\n",
    "- **Establish context**: SCM focus, SDK patterns, security emphasis  \n",
    "- **Guide responses**: Code examples, best practices, warnings\n",
    "\n",
    "**Message order in conversation:**\n",
    "```python\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a PAN-OS expert...\"),  # First: Set context\n",
    "    HumanMessage(content=\"How do I create a security rule?\"),  # User asks\n",
    "    AIMessage(content=\"Here's how to create a security rule...\"),  # AI responds\n",
    "    HumanMessage(content=\"What about NAT rules?\"),  # Follow-up\n",
    "    # ... conversation continues\n",
    "]\n",
    "```\n",
    "\n",
    "**Preview for Notebook 109:**\n",
    "\n",
    "In the next notebook, we'll use SystemMessage to:\n",
    "- Give our bot PAN-OS/SCM expertise\n",
    "- Improve response quality and relevance\n",
    "- Maintain consistent technical tone\n",
    "- Combine with conversation memory for powerful agents\n",
    "\n",
    "For now, we're keeping it simple - just HumanMessage and AIMessage!\n",
    "\n",
    "**Complete message type system:**\n",
    "- **SystemMessage**: Instructions/context for the AI (we'll use in 109)\n",
    "- **HumanMessage**: User input (we're using now)\n",
    "- **AIMessage**: AI responses (we're using now)\n",
    "- **ToolMessage**: Tool/function results (we'll use in 110)\n",
    "\n",
    "Let's continue building our simple bot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Defining the Agent State\n",
    "\n",
    "Just like in notebook 103, we need to define a state structure for our graph. This time, our state will hold **messages**.\n",
    "\n",
    "### State Structure\n",
    "\n",
    "Our state will have a single field:\n",
    "- `messages`: A list of HumanMessage objects\n",
    "\n",
    "### Why List[HumanMessage]?\n",
    "\n",
    "We use a list because:\n",
    "1. Users might send multiple messages\n",
    "2. We need to track message history (even though our simple bot won't use it)\n",
    "3. It's the foundation for conversation memory (coming in the next notebook)\n",
    "\n",
    "Let's define it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-definition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:50.728293Z",
     "iopub.status.busy": "2025-10-22T09:53:50.728215Z",
     "iopub.status.idle": "2025-10-22T09:53:50.730251Z",
     "shell.execute_reply": "2025-10-22T09:53:50.729873Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for our simple PAN-OS AI bot.\"\"\"\n",
    "    messages: List[HumanMessage]  # Only human messages - we don't save AI responses yet!\n",
    "\n",
    "print(\"âœ… AgentState defined!\")\n",
    "print(\"\\nState structure:\")\n",
    "print(\"  - messages: List[HumanMessage]\")\n",
    "print(\"\\nðŸ’¡ This state will hold all the messages users send to our bot\")\n",
    "print(\"\\nâš ï¸  Notice: We only track HumanMessage, not AIMessage!\")\n",
    "print(\"   This is intentional - our simple bot doesn't save AI responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acqpz8gy6n",
   "metadata": {},
   "source": [
    "### 4.1 Why Only HumanMessage?\n",
    "\n",
    "You might notice our state only tracks `HumanMessage` objects, not `AIMessage` responses.\n",
    "\n",
    "**This is intentional!** Our simple bot:\n",
    "- âœ… Receives user messages\n",
    "- âœ… Sends them to the LLM\n",
    "- âœ… Prints AI responses\n",
    "- âŒ **Doesn't save AI responses to state**\n",
    "\n",
    "This means **no conversation memory**. Each query is independent.\n",
    "\n",
    "**In Notebook 109, we'll upgrade to:**\n",
    "```python\n",
    "from typing import Union\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Union[HumanMessage, AIMessage]]  # Both types!\n",
    "```\n",
    "\n",
    "This will allow us to store the full conversation history (human messages AND AI responses), enabling memory.\n",
    "\n",
    "**For now, keep it simple** - just human messages, no AI response storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Initializing the LLM\n",
    "\n",
    "Now for the exciting part - initializing our Large Language Model!\n",
    "\n",
    "### Choosing a Model\n",
    "\n",
    "We'll use **Claude 3.5 Haiku** (Anthropic's fast, cost-effective model) for this workshop. You could also use:\n",
    "- **Claude 3.5 Sonnet**: More capable, slightly more expensive\n",
    "- **Claude 3 Opus**: Most capable, highest cost\n",
    "- **GPT-4 (OpenAI)**: Alternative provider via `ChatOpenAI`\n",
    "- **Ollama**: Local models (no API key needed)\n",
    "\n",
    "### Why ChatAnthropic?\n",
    "\n",
    "LangChain provides a unified interface through classes like:\n",
    "- `ChatAnthropic`: For Anthropic's Claude models\n",
    "- `ChatOpenAI`: For OpenAI's GPT models\n",
    "- `ChatOllama`: For local Ollama models\n",
    "\n",
    "They all work the same way, so switching providers is easy!\n",
    "\n",
    "### Personal Note\n",
    "\n",
    "I personally use `ChatAnthropic` because:\n",
    "- Excellent performance and reasoning\n",
    "- Very affordable (especially Haiku)\n",
    "- Strong privacy and safety features\n",
    "- Easy integration with LangChain\n",
    "\n",
    "Let's initialize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-init",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:50.731337Z",
     "iopub.status.busy": "2025-10-22T09:53:50.731257Z",
     "iopub.status.idle": "2025-10-22T09:53:50.733366Z",
     "shell.execute_reply": "2025-10-22T09:53:50.732955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM - this is where the AI magic happens!\n",
    "llm = ChatAnthropic(model=\"claude-3-5-haiku-20241022\")\n",
    "\n",
    "print(\"âœ… LLM initialized!\")\n",
    "print(\"\\nModel details:\")\n",
    "print(\"  - Provider: Anthropic\")\n",
    "print(\"  - Model: Claude 3.5 Haiku\")\n",
    "print(\"  - Cost: ~$0.25 per 1M input tokens, ~$1.25 per 1M output tokens\")\n",
    "print(\"\\nðŸ’¡ For more capable model, use 'claude-3-5-sonnet-20241022' instead!\")\n",
    "print(\"\\nâš ï¸  Make sure your ANTHROPIC_API_KEY is set in .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-llm-section",
   "metadata": {},
   "source": [
    "### 5.1 Quick LLM Test (Optional)\n",
    "\n",
    "Let's do a quick test to make sure our LLM is working before we build the full graph.\n",
    "\n",
    "**The invoke() Method:**\n",
    "\n",
    "You'll notice LangChain loves the word `invoke()`. We've used it to run graphs, and now we use it to call the LLM:\n",
    "- `graph.invoke()`: Run the graph\n",
    "- `llm.invoke()`: Call the LLM\n",
    "\n",
    "Consistent naming makes it easy to remember!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-test",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:50.734647Z",
     "iopub.status.busy": "2025-10-22T09:53:50.734545Z",
     "iopub.status.idle": "2025-10-22T09:53:55.159922Z",
     "shell.execute_reply": "2025-10-22T09:53:55.159100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick test: Ask the LLM a simple PAN-OS question\n",
    "test_message = HumanMessage(content=\"What is PAN-OS?\")\n",
    "test_response = llm.invoke([test_message])\n",
    "\n",
    "print(\"Test Question: What is PAN-OS?\")\n",
    "print(\"\\nAI Response:\")\n",
    "print(test_response.content)\n",
    "print(\"\\nâœ… LLM is working! Ready to build our agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fofxw6q7a4",
   "metadata": {},
   "source": [
    "### 5.2 Understanding AIMessage Responses\n",
    "\n",
    "When you call `llm.invoke()`, the response is not a plain string - it's an **AIMessage** object!\n",
    "\n",
    "**Why AIMessage?**\n",
    "- Structured representation of AI responses\n",
    "- Contains metadata (model used, tokens, timing)\n",
    "- Matches the HumanMessage pattern for consistency\n",
    "- Enables conversation history tracking (coming in notebook 109)\n",
    "\n",
    "Just like we use `HumanMessage` to represent messages FROM users TO the AI, we use `AIMessage` to represent messages FROM the AI TO users.\n",
    "\n",
    "**Message Flow:**\n",
    "```\n",
    "User â†’ HumanMessage â†’ LLM â†’ AIMessage â†’ Response\n",
    "```\n",
    "\n",
    "Let's examine the response object more closely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ivw88ob4j5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:55.162327Z",
     "iopub.status.busy": "2025-10-22T09:53:55.162055Z",
     "iopub.status.idle": "2025-10-22T09:53:55.166475Z",
     "shell.execute_reply": "2025-10-22T09:53:55.165812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the test response from the previous cell\n",
    "print(\"Response Object Analysis:\")\n",
    "print(f\"  Type: {type(test_response).__name__}\")\n",
    "print(f\"  Is AIMessage?: {isinstance(test_response, AIMessage)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AIMessage Structure:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nðŸ“ Content (what the AI said):\")\n",
    "print(f\"   {test_response.content[:150]}...\")  # First 150 chars\n",
    "\n",
    "print(f\"\\nðŸ·ï¸  Message Type:\")\n",
    "print(f\"   {test_response.type}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Response Metadata:\")\n",
    "for key, value in test_response.response_metadata.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ’¡ Key Insight:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"   HumanMessage: User â†’ AI\")\n",
    "print(\"   AIMessage:    AI â†’ User\")\n",
    "print(\"\\n   Both are message objects with .content and metadata!\")\n",
    "print(\"   This symmetry makes conversation tracking easy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-graph-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building the Agent Graph\n",
    "\n",
    "Now let's build our simple bot using everything we've learned!\n",
    "\n",
    "### The Process Node\n",
    "\n",
    "We'll create a node that:\n",
    "1. Takes the user's messages from state\n",
    "2. Sends them to the LLM using `invoke()`\n",
    "3. Gets the AI's response\n",
    "4. Prints the response (for now)\n",
    "\n",
    "### Graph Structure\n",
    "\n",
    "Remember from notebook 103? Our graph structure is:\n",
    "```\n",
    "START â†’ process_query â†’ END\n",
    "```\n",
    "\n",
    "Simple, but now with AI power!\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "node-function-section",
   "metadata": {},
   "source": [
    "### 6.1 Define the Process Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-node",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:55.168533Z",
     "iopub.status.busy": "2025-10-22T09:53:55.168406Z",
     "iopub.status.idle": "2025-10-22T09:53:55.172145Z",
     "shell.execute_reply": "2025-10-22T09:53:55.171447Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node: Process user query by sending it to the LLM.\n",
    "    \n",
    "    This is where the AI magic happens!\n",
    "    \n",
    "    Note: We print the response but DON'T save it to state.\n",
    "    This means no memory between queries (fixed in notebook 109).\n",
    "    \"\"\"\n",
    "    # Get the messages from state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Send messages to LLM and get response (returns AIMessage object)\n",
    "    # The llm.invoke() method calls Anthropic's API\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Print the AI's response content\n",
    "    print(\"\\nðŸ¤– AI Response:\")\n",
    "    print(response.content)\n",
    "    print()\n",
    "    \n",
    "    # âš ï¸ IMPORTANT: We return unchanged state (no memory!)\n",
    "    # We're NOT doing: state[\"messages\"].append(response)\n",
    "    # This means the bot won't remember this conversation\n",
    "    return state\n",
    "\n",
    "print(\"âœ… process_query node defined!\")\n",
    "print(\"\\nWhat this node does:\")\n",
    "print(\"  1. Reads messages from state\")\n",
    "print(\"  2. Invokes the LLM with those messages\")\n",
    "print(\"  3. Gets AIMessage response from Anthropic's servers\")\n",
    "print(\"  4. Prints the response content\")\n",
    "print(\"  5. Returns UNCHANGED state (no memory!)\")\n",
    "print(\"\\nðŸ’¡ The invoke() method handles all the API communication!\")\n",
    "print(\"\\nâš ï¸  The missing step: We never save the AI response!\")\n",
    "print(\"   This is why the bot has no memory (fixed in notebook 109)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ve9t1dhxzci",
   "metadata": {},
   "source": [
    "### 6.1.1 Production Pattern: Error Handling for LLM Calls\n",
    "\n",
    "Before we build our graph, let's learn about **production error handling** for LLM calls.\n",
    "\n",
    "### Why Error Handling Matters\n",
    "\n",
    "When calling external APIs like Anthropic, things can go wrong:\n",
    "- **Network failures**: Connection drops, timeouts\n",
    "- **API errors**: Rate limits, invalid API keys, service outages  \n",
    "- **Invalid responses**: Malformed data, unexpected formats\n",
    "\n",
    "In production SCM automation, you need **resilient bots** that handle these gracefully!\n",
    "\n",
    "### Common LLM API Errors\n",
    "\n",
    "**1. Invalid API Key**\n",
    "```\n",
    "Error: 401 Unauthorized - Invalid API key\n",
    "```\n",
    "Fix: Check your `.env` file and ANTHROPIC_API_KEY\n",
    "\n",
    "**2. Rate Limiting**\n",
    "```\n",
    "Error: 429 Too Many Requests - Rate limit exceeded\n",
    "```\n",
    "Fix: Implement retry logic with exponential backoff (like notebook 107!)\n",
    "\n",
    "**3. Network Failures**\n",
    "```\n",
    "Error: ConnectionError - Failed to reach API\n",
    "```\n",
    "Fix: Check internet connection, retry after delay\n",
    "\n",
    "**4. Timeout**\n",
    "```\n",
    "Error: TimeoutError - Request took too long\n",
    "```\n",
    "Fix: Increase timeout or retry\n",
    "\n",
    "### Error Handling Pattern\n",
    "\n",
    "Here's how to wrap `llm.invoke()` with production error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65luaw7kssa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:55.173939Z",
     "iopub.status.busy": "2025-10-22T09:53:55.173805Z",
     "iopub.status.idle": "2025-10-22T09:53:55.179314Z",
     "shell.execute_reply": "2025-10-22T09:53:55.178751Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_query_with_error_handling(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node: Process user query with production error handling.\n",
    "    \n",
    "    Handles common LLM API errors:\n",
    "    - ConnectionError: Network failures\n",
    "    - Timeout errors: Request took too long\n",
    "    - API errors: Rate limits, invalid keys, service outages\n",
    "    - Generic exceptions: Catch-all for unexpected issues\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    try:\n",
    "        # Call the LLM (this might fail!)\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        # Success! Print the response\n",
    "        print(\"\\nðŸ¤– AI Response:\")\n",
    "        print(response.content)\n",
    "        print()\n",
    "        \n",
    "    except ConnectionError as e:\n",
    "        # Network failure - can't reach Anthropic's API\n",
    "        print(\"\\nâŒ Connection Error:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        print(\"   â†’ Check your internet connection\")\n",
    "        print(\"   â†’ Try again in a moment\")\n",
    "        \n",
    "    except TimeoutError as e:\n",
    "        # Request took too long\n",
    "        print(\"\\nâŒ Timeout Error:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        print(\"   â†’ The LLM didn't respond in time\")\n",
    "        print(\"   â†’ Try a shorter query or retry\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catch-all for other errors (invalid API key, rate limits, etc.)\n",
    "        error_message = str(e)\n",
    "        \n",
    "        # Check for common error patterns\n",
    "        if \"401\" in error_message or \"unauthorized\" in error_message.lower():\n",
    "            print(\"\\nâŒ Authentication Error:\")\n",
    "            print(\"   Invalid API key\")\n",
    "            print(\"   â†’ Check your .env file\")\n",
    "            print(\"   â†’ Verify ANTHROPIC_API_KEY is correct\")\n",
    "            \n",
    "        elif \"429\" in error_message or \"rate limit\" in error_message.lower():\n",
    "            print(\"\\nâŒ Rate Limit Error:\")\n",
    "            print(\"   Too many requests too quickly\")\n",
    "            print(\"   â†’ Wait a moment and try again\")\n",
    "            print(\"   â†’ Consider implementing retry logic (notebook 107!)\")\n",
    "            \n",
    "        else:\n",
    "            # Unknown error\n",
    "            print(\"\\nâŒ Unexpected Error:\")\n",
    "            print(f\"   {error_message}\")\n",
    "            print(\"   â†’ Check the error message above\")\n",
    "            print(\"   â†’ Verify your API key and network connection\")\n",
    "    \n",
    "    # Always return state (even if error occurred)\n",
    "    return state\n",
    "\n",
    "print(\"âœ… process_query_with_error_handling node defined!\")\n",
    "print(\"\\nWhat this node does:\")\n",
    "print(\"  1. Wraps llm.invoke() in try-catch block\")\n",
    "print(\"  2. Handles ConnectionError (network failures)\")\n",
    "print(\"  3. Handles TimeoutError (slow responses)\")\n",
    "print(\"  4. Handles API errors (401 auth, 429 rate limits)\")\n",
    "print(\"  5. Provides helpful error messages for debugging\")\n",
    "print(\"  6. Always returns state (graceful degradation)\")\n",
    "print(\"\\nðŸ’¡ This is production-ready error handling!\")\n",
    "print(\"\\nâš ï¸  For this notebook, we'll use the simpler process_query\")\n",
    "print(\"   But in production SCM automation, use this pattern!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-compile-section",
   "metadata": {},
   "source": [
    "### 6.2 Build and Compile the Graph\n",
    "\n",
    "This should look familiar from notebooks 103-107!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:55.180602Z",
     "iopub.status.busy": "2025-10-22T09:53:55.180507Z",
     "iopub.status.idle": "2025-10-22T09:53:55.184064Z",
     "shell.execute_reply": "2025-10-22T09:53:55.183562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add the process node\n",
    "graph.add_node(\"process_query\", process_query)\n",
    "\n",
    "# Set entry and exit points\n",
    "graph.set_entry_point(\"process_query\")\n",
    "graph.set_finish_point(\"process_query\")\n",
    "\n",
    "# Compile the graph\n",
    "agent = graph.compile()\n",
    "\n",
    "print(\"âœ… Graph built and compiled!\")\n",
    "print(\"\\nGraph structure:\")\n",
    "print(\"  START â†’ process_query â†’ END\")\n",
    "print(\"\\nðŸ’¡ Same structure as notebook 103, but now with LLM power!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-section",
   "metadata": {},
   "source": [
    "### 6.3 Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-graph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:55.185533Z",
     "iopub.status.busy": "2025-10-22T09:53:55.185436Z",
     "iopub.status.idle": "2025-10-22T09:53:55.268549Z",
     "shell.execute_reply": "2025-10-22T09:53:55.268056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: It looks like our first graph from notebook 103!\")\n",
    "print(\"   But now the 'process_query' node contains AI magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Testing the Simple Bot\n",
    "\n",
    "Time to test our AI agent! We'll start with a single question, then try multiple questions.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. User enters a question about PAN-OS\n",
    "2. We wrap it in a `HumanMessage`\n",
    "3. We invoke the graph with the message\n",
    "4. The LLM processes it and responds\n",
    "\n",
    "Let's try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-test-section",
   "metadata": {},
   "source": [
    "### 7.1 Single Query Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-test-section",
   "metadata": {},
   "source": [
    "### 7.2 Multiple Queries with While Loop\n",
    "\n",
    "Let's make it more interactive! We'll use a while loop so you can ask multiple questions.\n",
    "\n",
    "**How to exit:** Type `exit` or `quit` when done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-queries-section",
   "metadata": {},
   "source": [
    "### 7.3 Example Queries to Try\n",
    "\n",
    "Here are some PAN-OS questions to try with your bot:\n",
    "\n",
    "**Upgrade Questions:**\n",
    "- \"What is the recommended upgrade path from PAN-OS 10.1 to 10.2?\"\n",
    "- \"How do I check if my firewall is ready for an upgrade?\"\n",
    "- \"What are the steps for upgrading an HA pair?\"\n",
    "\n",
    "**Configuration Questions:**\n",
    "- \"How do I create a security policy rule?\"\n",
    "- \"What's the difference between a security zone and a zone protection profile?\"\n",
    "- \"How do I configure NAT on a PAN-OS firewall?\"\n",
    "\n",
    "**Troubleshooting Questions:**\n",
    "- \"What does error code 'session discard' mean?\"\n",
    "- \"How do I check CPU and memory usage on PAN-OS?\"\n",
    "- \"Why is my firewall dropping traffic?\"\n",
    "\n",
    "**Strata Cloud Manager (SCM) Specific Questions:**\n",
    "- \"How do I create an address object in Strata Cloud Manager using pan-scm-sdk?\"\n",
    "- \"What's the difference between folder, snippet, and device containers in SCM?\"\n",
    "- \"How do I list all security policies in a specific folder using the SDK?\"\n",
    "- \"What are the required fields for creating a security rule in SCM?\"\n",
    "- \"How do I handle pagination when listing address objects in SCM?\"\n",
    "- \"What's the best practice for organizing SCM configuration across folders?\"\n",
    "\n",
    "**Try asking follow-up questions and see what happens!**\n",
    "\n",
    "ðŸ’¡ **Hint**: Pay special attention to whether the bot remembers context from your previous questions. You'll notice it doesn't - that's the memory problem we'll solve in the next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sd56s4geu8",
   "metadata": {},
   "source": [
    "### 7.4 Bot Providing SCM SDK Code Examples\n",
    "\n",
    "One powerful use case for our bot: **generating pan-scm-sdk code snippets**!\n",
    "\n",
    "Even without memory, the bot can provide working SDK examples based on your questions. Let's try a few SCM-specific queries and see the bot generate actual code you can use.\n",
    "\n",
    "**How this helps:**\n",
    "- Get instant SDK examples without searching documentation\n",
    "- Learn correct pan-scm-sdk patterns\n",
    "- Copy working code directly into your automation scripts\n",
    "- Understand required parameters and data structures\n",
    "\n",
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yyzvnnyzvp9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:55.270140Z",
     "iopub.status.busy": "2025-10-22T09:53:55.270029Z",
     "iopub.status.idle": "2025-10-22T09:54:33.591243Z",
     "shell.execute_reply": "2025-10-22T09:54:33.586801Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SCM SDK CODE GENERATION DEMO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Example 1: Creating an address object\n",
    "print(\"\\nðŸ“ Example 1: Creating an Address Object\")\n",
    "print(\"-\" * 70)\n",
    "query1 = HumanMessage(content=\"Show me how to create an address object using pan-scm-sdk with the following: name 'web-server-01', IP address '10.1.1.100/32', folder 'Texas', and description 'Production web server'\")\n",
    "agent.invoke({\"messages\": [query1]})\n",
    "\n",
    "# Example 2: Listing security rules with pagination\n",
    "print(\"\\nðŸ“ Example 2: Listing Security Rules with Pagination\")\n",
    "print(\"-\" * 70)\n",
    "query2 = HumanMessage(content=\"Show me pan-scm-sdk code to list all security rules in the 'Texas' folder with proper pagination handling\")\n",
    "agent.invoke({\"messages\": [query2]})\n",
    "\n",
    "# Example 3: Error handling for SCM operations\n",
    "print(\"\\nðŸ“ Example 3: SCM Exception Handling\")\n",
    "print(\"-\" * 70)\n",
    "query3 = HumanMessage(content=\"Show me how to handle exceptions when creating an address object in pan-scm-sdk, including InvalidObjectError and NameNotUniqueError\")\n",
    "agent.invoke({\"messages\": [query3]})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ’¡ KEY INSIGHT: LLM-Generated SDK Code\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThe bot provided working pan-scm-sdk code snippets!\")\n",
    "print(\"\\nâœ… Benefits:\")\n",
    "print(\"   - Instant SDK examples without documentation search\")\n",
    "print(\"   - Correct import statements and client initialization\")\n",
    "print(\"   - Proper data models and required fields\")\n",
    "print(\"   - Exception handling patterns\")\n",
    "print(\"   - Ready to copy-paste into your automation scripts\")\n",
    "print(\"\\nâš ï¸  Remember: Always review and test generated code!\")\n",
    "print(\"   - Verify folder names match your environment\")\n",
    "print(\"   - Check API credentials are correctly loaded\")\n",
    "print(\"   - Test in development before production\")\n",
    "print(\"\\nðŸ”— Reference: See docs/examples/ for more SDK patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-problem-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. The Memory Problem\n",
    "\n",
    "Now let's discover the critical limitation of our simple bot.\n",
    "\n",
    "### The Experiment\n",
    "\n",
    "Try this sequence of questions:\n",
    "\n",
    "1. **First message:** \"My firewall hostname is fw-prod-01\"\n",
    "2. **Second message:** \"What is my firewall's hostname?\"\n",
    "\n",
    "### What Happens?\n",
    "\n",
    "The bot will respond with something like: \"I don't have information about your firewall's hostname.\"\n",
    "\n",
    "**Why?** Because our bot has **no memory**. Each question is processed independently with no context from previous messages.\n",
    "\n",
    "### The Code Explanation\n",
    "\n",
    "Look at our `process_query` node:\n",
    "\n",
    "```python\n",
    "def process_query(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    # We print the response but DON'T save it to state!\n",
    "    return state  # State unchanged - no memory of the conversation\n",
    "```\n",
    "\n",
    "**The problem:** We never update the state with:\n",
    "- The AI's response\n",
    "- The conversation history\n",
    "- Any context from previous messages\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "For real-world PAN-OS automation, you need:\n",
    "- Multi-turn troubleshooting conversations\n",
    "- Context-aware recommendations\n",
    "- Ability to reference previous messages\n",
    "\n",
    "### The Solution (Coming Next!)\n",
    "\n",
    "In the next notebook, we'll learn about:\n",
    "- **Conversation memory**: Storing message history\n",
    "- **State reducers**: The `Annotated[list, add_messages]` pattern\n",
    "- **True conversational agents**: Bots that remember context\n",
    "\n",
    "But for now, you've successfully integrated an LLM into LangGraph - that's a huge milestone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-demo-section",
   "metadata": {},
   "source": [
    "### 8.1 Demonstrate the Memory Problem\n",
    "\n",
    "Let's run a concrete example showing the limitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:33.596251Z",
     "iopub.status.busy": "2025-10-22T09:54:33.596016Z",
     "iopub.status.idle": "2025-10-22T09:54:38.586780Z",
     "shell.execute_reply": "2025-10-22T09:54:38.585400Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATING THE MEMORY PROBLEM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First message: Tell the bot our firewall name\n",
    "print(\"\\n1ï¸âƒ£ First Query: 'My firewall hostname is fw-prod-01'\")\n",
    "message1 = HumanMessage(content=\"My firewall hostname is fw-prod-01\")\n",
    "agent.invoke({\"messages\": [message1]})\n",
    "\n",
    "# Second message: Ask what the firewall name is\n",
    "print(\"\\n2ï¸âƒ£ Second Query: 'What is my firewall hostname?'\")\n",
    "message2 = HumanMessage(content=\"What is my firewall hostname?\")\n",
    "agent.invoke({\"messages\": [message2]})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âŒ PROBLEM: The bot doesn't remember we told it 'fw-prod-01'!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWhy? Because we never stored the conversation history in state.\")\n",
    "print(\"Each query is independent - no memory between messages.\")\n",
    "print(\"\\nðŸ’¡ Next notebook: We'll fix this with conversation memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've built your first AI-powered LangGraph agent!\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Integrated an LLM** - Connected Anthropic's Claude to our graph\n",
    "2. **Used HumanMessage** - Learned how LangChain structures messages\n",
    "3. **Built a Simple Bot** - Created a single-node graph with AI capabilities\n",
    "4. **Tested with PAN-OS Queries** - Asked real firewall questions\n",
    "5. **Discovered the Memory Problem** - Learned why stateless bots are limited\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "âœ… **LangChain + LangGraph Work Together**\n",
    "   - LangChain: LLM tools (ChatAnthropic, messages)\n",
    "   - LangGraph: Workflow orchestration (graphs, nodes)\n",
    "\n",
    "âœ… **HumanMessage Objects**\n",
    "   - Structured way to represent user messages\n",
    "   - Better than plain strings for conversation management\n",
    "\n",
    "âœ… **The invoke() Pattern**\n",
    "   - `llm.invoke()`: Call the LLM\n",
    "   - `agent.invoke()`: Run the graph\n",
    "   - Consistent naming across LangChain/LangGraph\n",
    "\n",
    "âœ… **State Management**\n",
    "   - Defined `AgentState` with `List[HumanMessage]`\n",
    "   - State flows through the graph like before\n",
    "\n",
    "âœ… **The Memory Problem**\n",
    "   - Simple bots don't remember previous messages\n",
    "   - Each query is independent\n",
    "   - This motivates the next notebook!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Even though our bot lacks memory, you've learned the fundamental pattern for integrating LLMs into LangGraph:\n",
    "\n",
    "```python\n",
    "# 1. Define state with messages\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[HumanMessage]\n",
    "\n",
    "# 2. Initialize LLM\n",
    "llm = ChatAnthropic(model=\"claude-3-5-haiku-20241022\")\n",
    "\n",
    "# 3. Create node that invokes LLM\n",
    "def process(state):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return state\n",
    "\n",
    "# 4. Build graph as usual\n",
    "graph.add_node(\"process\", process)\n",
    "```\n",
    "\n",
    "This pattern is the foundation for ALL LLM agents in LangGraph!\n",
    "\n",
    "### Real-World Use Cases (Without Memory)\n",
    "\n",
    "Even simple bots are useful for:\n",
    "- **One-off queries**: \"What's the CLI command for X?\"\n",
    "- **Quick lookups**: \"What does error code Y mean?\"\n",
    "- **Documentation search**: \"How do I configure Z?\"\n",
    "- **Best practices**: \"What's the recommended approach for X?\"\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebook (**109: Conversational Memory**), we'll learn:\n",
    "- How to add **conversation memory**\n",
    "- The `Union[HumanMessage, AIMessage]` **pattern**\n",
    "- **Multi-turn conversations** with context\n",
    "- **AIMessage** for storing LLM responses\n",
    "- Building **true conversational agents**\n",
    "\n",
    "### Code Summary\n",
    "\n",
    "Our complete simple bot in ~25 lines:\n",
    "\n",
    "```python\n",
    "# State\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[HumanMessage]\n",
    "\n",
    "# LLM\n",
    "llm = ChatAnthropic(model=\"claude-3-5-haiku-20241022\")\n",
    "\n",
    "# Node\n",
    "def process_query(state: AgentState) -> AgentState:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    print(response.content)\n",
    "    return state\n",
    "\n",
    "# Graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"process_query\", process_query)\n",
    "graph.set_entry_point(\"process_query\")\n",
    "graph.set_finish_point(\"process_query\")\n",
    "agent = graph.compile()\n",
    "\n",
    "# Run\n",
    "agent.invoke({\"messages\": [HumanMessage(content=\"Hi!\")]})\n",
    "```\n",
    "\n",
    "Simple, clean, and powerful!\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "You've crossed a major milestone - you can now integrate AI into your workflows! The journey from here gets even more exciting as we add memory, tools, and sophisticated agent behaviors.\n",
    "\n",
    "**Great work!** ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "### Next Workshop Preview\n",
    "\n",
    "In **Notebook 109: Conversational Memory**, we'll solve the memory problem:\n",
    "\n",
    "```\n",
    "You: My firewall is fw-prod-01\n",
    "AI: Got it! I'll remember that your firewall is fw-prod-01.\n",
    "\n",
    "You: What's my firewall's hostname?\n",
    "AI: Your firewall's hostname is fw-prod-01.\n",
    "```\n",
    "\n",
    "See you there! ðŸ’¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
