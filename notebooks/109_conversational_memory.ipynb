{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# 109 LangGraph: Conversational Memory (Manual Management)\n",
    "\n",
    "**Workshop**: LangGraph 109\n",
    "**Duration**: ~45 minutes\n",
    "**Difficulty**: Intermediate\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Use both `HumanMessage` and `AIMessage` types for conversation tracking\n",
    "- Implement conversation memory using manual history management\n",
    "- Understand the `Union` type for handling multiple message types\n",
    "- Build a stateful conversation loop that remembers context\n",
    "- Learn about conversation persistence (saving to files/databases)\n",
    "- Discover the cost implications of growing conversation history\n",
    "- Implement conversation history trimming strategies\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Knowledge**: Completed notebook 108 (First LLM Integration)\n",
    "- **Understanding**: Union types from notebook 101, graph patterns from 103-107\n",
    "- **Setup**: Anthropic API key configured\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "We're fixing the memory problem from notebook 108! This time, our bot will:\n",
    "- Remember previous messages in the conversation\n",
    "- Track both human questions AND AI responses\n",
    "- Maintain context across multiple turns\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "You: My firewall is fw-prod-01\n",
    "AI: Got it! How can I help with fw-prod-01?\n",
    "\n",
    "You: What's my firewall's hostname?\n",
    "AI: Your firewall's hostname is fw-prod-01.\n",
    "```\n",
    "\n",
    "**Graph Structure:** (Same as before, but with memory!)\n",
    "```\n",
    "START ‚Üí process_query ‚Üí END\n",
    "```\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Setup and New Imports](#2-setup-and-new-imports)\n",
    "3. [Understanding AIMessage and Union Types](#3-understanding-aimessage-and-union-types)\n",
    "4. [Building the Memory-Enabled State](#4-building-the-memory-enabled-state)\n",
    "5. [Creating the Conversation Node](#5-creating-the-conversation-node)\n",
    "6. [Implementing the Conversation Loop](#6-implementing-the-conversation-loop)\n",
    "7. [Testing with Memory](#7-testing-with-memory)\n",
    "8. [Problem 1: Conversation Persistence](#8-problem-1-conversation-persistence)\n",
    "9. [Problem 2: Growing Token Costs](#9-problem-2-growing-token-costs)\n",
    "10. [Summary](#10-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5ytdxj0w1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Welcome back! In notebook 108, we built a simple AI bot that had one critical flaw: **it couldn't remember anything**.\n",
    "\n",
    "### The Problem We're Solving\n",
    "\n",
    "Remember this from last time?\n",
    "\n",
    "```\n",
    "You: My firewall hostname is fw-prod-01\n",
    "AI: Thanks for letting me know!\n",
    "\n",
    "You: What is my firewall hostname?\n",
    "AI: I don't have information about your firewall hostname.\n",
    "```\n",
    "\n",
    "**Why?** Each query was independent - we never stored the conversation history.\n",
    "\n",
    "### The Solution: Conversation Memory\n",
    "\n",
    "In this notebook, we'll implement **manual conversation memory** by:\n",
    "1. Storing both `HumanMessage` and `AIMessage` objects\n",
    "2. Maintaining a conversation history list\n",
    "3. Sending the entire conversation to the LLM each time\n",
    "4. Updating the history with each exchange\n",
    "\n",
    "### Why Manual First?\n",
    "\n",
    "Before we learn the advanced `Annotated[list, add_messages]` reducer pattern (coming in notebook 110), we'll build memory manually. This helps you understand:\n",
    "- How conversation history actually works\n",
    "- Why reducers are so helpful (you'll appreciate them more!)\n",
    "- The cost implications of growing histories\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "With memory, our PAN-OS bot can handle:\n",
    "- Multi-turn troubleshooting: \"My firewall is dropping traffic\" ‚Üí \"Check logs\" ‚Üí \"What should I look for?\"\n",
    "- Context-aware recommendations: \"I'm on 10.1\" ‚Üí \"Should I upgrade?\" ‚Üí \"What's the path to 11.0?\"\n",
    "- Configuration assistance: \"I need NAT\" ‚Üí \"For what source?\" ‚Üí \"How do I configure it?\"\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Setup and New Imports\n",
    "\n",
    "### What's New?\n",
    "\n",
    "Compared to notebook 108, we're adding:\n",
    "1. **AIMessage**: To represent messages from the AI\n",
    "2. **Union**: Type annotation for handling multiple message types\n",
    "\n",
    "### Quick Union Refresher\n",
    "\n",
    "Remember from notebook 101? `Union` lets a variable accept multiple types:\n",
    "\n",
    "```python\n",
    "Union[HumanMessage, AIMessage]  # Can be EITHER type\n",
    "```\n",
    "\n",
    "This is perfect for conversation history where we have both human and AI messages!\n",
    "\n",
    "### Note About AI Agentic Libraries\n",
    "\n",
    "**Important insight:** You could build AI agents with pure Python functions - you don't technically need LangChain or LangGraph!\n",
    "\n",
    "However, I recommend using these libraries because:\n",
    "- **LangGraph**: Great balance of control vs. convenience\n",
    "- **Reduces boilerplate**: Handles a lot of tedious code for you\n",
    "- **Battle-tested**: Robust implementations of common patterns\n",
    "- **Flexibility**: More control than alternatives like CrewAI or Autogen\n",
    "\n",
    "Think of LangGraph as the sweet spot between \"total control\" (pure Python) and \"total convenience\" (high-level frameworks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.392987Z",
     "iopub.status.busy": "2025-10-22T09:54:40.392819Z",
     "iopub.status.idle": "2025-10-22T09:54:40.655796Z",
     "shell.execute_reply": "2025-10-22T09:54:40.655349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core typing imports\n",
    "from typing import TypedDict, List, Union\n",
    "\n",
    "# LangChain message types - NOW INCLUDING AIMessage!\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain LLM integration - Using Anthropic Claude\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# LangGraph core\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Environment variable loading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"\\nüÜï What's new in this notebook:\")\n",
    "print(\"  - AIMessage: Represents messages FROM the AI to users\")\n",
    "print(\"  - Union: Allows storing BOTH HumanMessage AND AIMessage\")\n",
    "print(\"\\nüí° Union[HumanMessage, AIMessage] means:\")\n",
    "print(\"   'This can be either a HumanMessage OR an AIMessage'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-env",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.656940Z",
     "iopub.status.busy": "2025-10-22T09:54:40.656853Z",
     "iopub.status.idle": "2025-10-22T09:54:40.659040Z",
     "shell.execute_reply": "2025-10-22T09:54:40.658688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aimessage-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding AIMessage and Union Types\n",
    "\n",
    "### Message Types Recap\n",
    "\n",
    "LangChain provides different message types for different speakers:\n",
    "\n",
    "| Type | Purpose | Example |\n",
    "|------|---------|----------|\n",
    "| `HumanMessage` | User to AI | \"What's the upgrade path?\" |\n",
    "| `AIMessage` | AI to User | \"You should go 10.1 ‚Üí 10.2 ‚Üí 11.0\" |\n",
    "| `SystemMessage` | Instructions | \"You are a PAN-OS expert\" |\n",
    "| `ToolMessage` | Tool results | \"API returned: success\" |\n",
    "\n",
    "### Why AIMessage?\n",
    "\n",
    "In notebook 108, we only tracked `HumanMessage` - the user's questions. But for conversation memory, we need to track BOTH sides:\n",
    "- What the user asked (HumanMessage)\n",
    "- What the AI responded (AIMessage)\n",
    "\n",
    "### Using Union for Multiple Types\n",
    "\n",
    "We could create two separate lists:\n",
    "\n",
    "```python\n",
    "# ‚ùå Naive approach - separate lists\n",
    "class BadState(TypedDict):\n",
    "    human_messages: List[HumanMessage]\n",
    "    ai_messages: List[AIMessage]\n",
    "```\n",
    "\n",
    "But this is messy! How do we know which AI message responds to which human message?\n",
    "\n",
    "**Better approach** - single list with Union:\n",
    "\n",
    "```python\n",
    "# ‚úÖ Better - single list with both types\n",
    "class GoodState(TypedDict):\n",
    "    messages: List[Union[HumanMessage, AIMessage]]\n",
    "```\n",
    "\n",
    "Now messages stay in order: Human, AI, Human, AI, Human, AI...\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.660133Z",
     "iopub.status.busy": "2025-10-22T09:54:40.660070Z",
     "iopub.status.idle": "2025-10-22T09:54:40.662677Z",
     "shell.execute_reply": "2025-10-22T09:54:40.662268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create both message types\n",
    "human_msg = HumanMessage(content=\"What is the upgrade path from PAN-OS 10.1 to 10.2?\")\n",
    "ai_msg = AIMessage(content=\"The upgrade path is: 10.1.0 ‚Üí 10.1.latest ‚Üí 10.2.0 ‚Üí 10.2.latest\")\n",
    "\n",
    "print(\"HumanMessage:\")\n",
    "print(f\"  Type: {type(human_msg).__name__}\")\n",
    "print(f\"  Content: {human_msg.content}\")\n",
    "print()\n",
    "print(\"AIMessage:\")\n",
    "print(f\"  Type: {type(ai_msg).__name__}\")\n",
    "print(f\"  Content: {ai_msg.content}\")\n",
    "print()\n",
    "\n",
    "# Store them together in a list\n",
    "conversation = [human_msg, ai_msg]\n",
    "print(\"Conversation history:\")\n",
    "for i, msg in enumerate(conversation, 1):\n",
    "    speaker = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "    print(f\"  {i}. [{speaker}] {msg.content}\")\n",
    "\n",
    "print(\"\\nüí° Notice: Both types coexist in the same list!\")\n",
    "print(\"   This preserves conversation order perfectly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Building the Memory-Enabled State\n",
    "\n",
    "### State Comparison\n",
    "\n",
    "**Notebook 108 (No Memory):**\n",
    "```python\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[HumanMessage]  # Only human messages\n",
    "```\n",
    "\n",
    "**Notebook 109 (With Memory):**\n",
    "```python\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Union[HumanMessage, AIMessage]]  # BOTH types!\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The Union type tells Python: \"This list can contain a mix of HumanMessage and AIMessage objects.\"\n",
    "\n",
    "This single change enables conversation memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-definition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.663628Z",
     "iopub.status.busy": "2025-10-22T09:54:40.663549Z",
     "iopub.status.idle": "2025-10-22T09:54:40.665754Z",
     "shell.execute_reply": "2025-10-22T09:54:40.665463Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for our memory-enabled PAN-OS AI bot.\"\"\"\n",
    "    messages: List[Union[HumanMessage, AIMessage]]  # Can store BOTH message types\n",
    "\n",
    "print(\"‚úÖ AgentState defined with memory support!\")\n",
    "print(\"\\nState structure:\")\n",
    "print(\"  - messages: List[Union[HumanMessage, AIMessage]]\")\n",
    "print(\"\\nüí° This state can now track full conversations:\")\n",
    "print(\"   [HumanMessage, AIMessage, HumanMessage, AIMessage, ...]\")\n",
    "print(\"\\nüìù NOTE: Modern Python 3.10+ Syntax Alternative:\")\n",
    "print(\"   messages: list[HumanMessage | AIMessage]  # Equivalent, more concise!\")\n",
    "print(\"\\n   Both are valid:\")\n",
    "print(\"   ‚úì List[Union[HumanMessage, AIMessage]]  # Python 3.7+ (more compatible)\")\n",
    "print(\"   ‚úì list[HumanMessage | AIMessage]        # Python 3.10+ (more modern)\")\n",
    "print(\"\\n   This notebook uses the older syntax for broader compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-init",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.666629Z",
     "iopub.status.busy": "2025-10-22T09:54:40.666572Z",
     "iopub.status.idle": "2025-10-22T09:54:40.668270Z",
     "shell.execute_reply": "2025-10-22T09:54:40.667936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM - Using Claude\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0)\n",
    "\n",
    "print(\"‚úÖ LLM initialized!\")\n",
    "print(\"   Model: Claude 3.5 Sonnet\")\n",
    "print(\"   Provider: Anthropic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68tzrx3z3r3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Creating the Conversation Node\n",
    "\n",
    "### The Key Difference from Notebook 108\n",
    "\n",
    "**Notebook 108:**\n",
    "```python\n",
    "def process_query(state: AgentState) -> AgentState:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    print(response.content)\n",
    "    return state  # ‚ùå Doesn't save AI response\n",
    "```\n",
    "\n",
    "**Notebook 109:**\n",
    "```python\n",
    "def process_query(state: AgentState) -> AgentState:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))  # ‚≠ê SAVES!\n",
    "    print(response.content)\n",
    "    return state\n",
    "```\n",
    "\n",
    "### What's Happening Here?\n",
    "\n",
    "1. **Invoke LLM**: Send all messages (human + AI history) to the model\n",
    "2. **Get Response**: LLM returns an AIMessage\n",
    "3. **‚≠ê Append to State**: Add the AI response to the messages list\n",
    "4. **Return State**: Updated state now includes the AI response\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "By appending `AIMessage(content=response.content)` to the state, we're building a conversation history:\n",
    "\n",
    "```\n",
    "Turn 1: [HumanMessage(\"My firewall is fw-prod-01\")]\n",
    "        [HumanMessage(\"My firewall is fw-prod-01\"), AIMessage(\"Got it!\")]\n",
    "\n",
    "Turn 2: [HumanMessage(\"My firewall is fw-prod-01\"), AIMessage(\"Got it!\"), HumanMessage(\"What's my hostname?\")]\n",
    "        [HumanMessage(\"My firewall is fw-prod-01\"), AIMessage(\"Got it!\"), HumanMessage(\"What's my hostname?\"), AIMessage(\"It's fw-prod-01\")]\n",
    "```\n",
    "\n",
    "The LLM sees the ENTIRE conversation each time, so it can reference previous context!\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iinw4p8x49q",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.669192Z",
     "iopub.status.busy": "2025-10-22T09:54:40.669135Z",
     "iopub.status.idle": "2025-10-22T09:54:40.671278Z",
     "shell.execute_reply": "2025-10-22T09:54:40.670955Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Process the conversation with memory support.\n",
    "    \n",
    "    This node:\n",
    "    1. Receives all messages (human + AI history)\n",
    "    2. Invokes the LLM with the full conversation\n",
    "    3. Appends the AI response to the state\n",
    "    4. Returns the updated state\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state with message history\n",
    "    \n",
    "    Returns:\n",
    "        Updated state with AI response appended\n",
    "    \n",
    "    Note on State Mutation:\n",
    "        This function uses state[\"messages\"].append() to mutate state directly.\n",
    "        An alternative functional approach would be:\n",
    "            return {\"messages\": state[\"messages\"] + [AIMessage(...)]}\n",
    "        \n",
    "        We use mutation here for clarity and simplicity when teaching manual \n",
    "        memory management. In notebook 110, you'll learn about reducers which\n",
    "        make this choice less important - the reducer handles merging automatically!\n",
    "    \"\"\"\n",
    "    # Get the full conversation history\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Invoke LLM with all messages\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # ‚≠ê KEY DIFFERENCE: Append AI response to state\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))\n",
    "    \n",
    "    # Show the response\n",
    "    print(f\"\\nü§ñ AI: {response.content}\\n\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Conversation node defined with memory!\")\n",
    "print(\"\\nüí° Key insight:\")\n",
    "print(\"   state['messages'].append(AIMessage(...)) enables memory\")\n",
    "print(\"   Each turn adds to the growing conversation history\")\n",
    "print(\"\\nüìù State Mutation vs. Returning Updates:\")\n",
    "print(\"   This function mutates state directly for pedagogical clarity.\")\n",
    "print(\"   Alternative: return {'messages': state['messages'] + [AIMessage(...)]}\")\n",
    "print(\"   Both work! Reducers (notebook 110) make this choice less critical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7ok2e95zes",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.672116Z",
     "iopub.status.busy": "2025-10-22T09:54:40.672060Z",
     "iopub.status.idle": "2025-10-22T09:54:40.674496Z",
     "shell.execute_reply": "2025-10-22T09:54:40.674200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the graph (same structure as 108, different behavior!)\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add the conversation node\n",
    "graph.add_node(\"process_query\", process_query)\n",
    "\n",
    "# Define the flow\n",
    "graph.add_edge(START, \"process_query\")\n",
    "graph.add_edge(\"process_query\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = graph.compile()\n",
    "\n",
    "print(\"‚úÖ Conversational agent graph compiled!\")\n",
    "print(\"\\nGraph structure:\")\n",
    "print(\"  START ‚Üí process_query ‚Üí END\")\n",
    "print(\"\\nüí° Same structure as notebook 108, but now with memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2qdaujr2gio",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.1 Production Pattern: Error Handling\n",
    "\n",
    "While our focus is on memory management, production LLM applications need robust error handling. Let's add a version of `process_query` with proper exception handling.\n",
    "\n",
    "### Common LLM API Errors\n",
    "\n",
    "| Error Type | Cause | Handling Strategy |\n",
    "|------------|-------|-------------------|\n",
    "| `ConnectionError` | Network issues | Retry with backoff |\n",
    "| `TimeoutError` | API timeout | Retry or use shorter context |\n",
    "| HTTP 401 | Invalid API key | Check credentials |\n",
    "| HTTP 429 | Rate limit exceeded | Implement retry logic |\n",
    "| HTTP 500 | Server error | Retry with backoff |\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "In production SCM automation:\n",
    "- Network interruptions shouldn't crash workflows\n",
    "- Rate limits need graceful handling\n",
    "- Users need clear error messages\n",
    "- Failed LLM calls shouldn't lose conversation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ahmk82k2j",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.675605Z",
     "iopub.status.busy": "2025-10-22T09:54:40.675540Z",
     "iopub.status.idle": "2025-10-22T09:54:40.679527Z",
     "shell.execute_reply": "2025-10-22T09:54:40.679154Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_query_with_error_handling(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Production-ready conversation node with comprehensive error handling.\n",
    "    \n",
    "    This demonstrates how to handle common LLM API failures gracefully\n",
    "    while preserving conversation state and providing clear user feedback.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state with message history\n",
    "    \n",
    "    Returns:\n",
    "        Updated state (with AI response on success, or error message on failure)\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    try:\n",
    "        # Attempt LLM invocation\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        # Success - append AI response\n",
    "        state[\"messages\"].append(AIMessage(content=response.content))\n",
    "        print(f\"\\nü§ñ AI: {response.content}\\n\")\n",
    "        \n",
    "    except ConnectionError as e:\n",
    "        # Network connectivity issues\n",
    "        error_msg = f\"‚ùå Connection Error: {str(e)}\"\n",
    "        print(f\"\\n{error_msg}\")\n",
    "        print(\"   ‚Üí Check your internet connection and try again\")\n",
    "        \n",
    "        # Optionally append error to conversation for context\n",
    "        state[\"messages\"].append(AIMessage(\n",
    "            content=\"[Connection error - unable to process request]\"\n",
    "        ))\n",
    "        \n",
    "    except TimeoutError as e:\n",
    "        # API timeout (request took too long)\n",
    "        error_msg = f\"‚ùå Timeout Error: {str(e)}\"\n",
    "        print(f\"\\n{error_msg}\")\n",
    "        print(\"   ‚Üí The LLM didn't respond in time\")\n",
    "        print(\"   ‚Üí Try reducing conversation history or use trimming\")\n",
    "        \n",
    "        state[\"messages\"].append(AIMessage(\n",
    "            content=\"[Timeout error - request took too long]\"\n",
    "        ))\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catch-all for other API errors\n",
    "        error_message = str(e)\n",
    "        \n",
    "        # Check for specific HTTP error codes in the message\n",
    "        if \"401\" in error_message or \"unauthorized\" in error_message.lower():\n",
    "            print(\"\\n‚ùå Authentication Error:\")\n",
    "            print(\"   ‚Üí Check your ANTHROPIC_API_KEY in .env file\")\n",
    "            print(\"   ‚Üí Verify the API key is valid and active\")\n",
    "            \n",
    "            state[\"messages\"].append(AIMessage(\n",
    "                content=\"[Authentication error - check API credentials]\"\n",
    "            ))\n",
    "            \n",
    "        elif \"429\" in error_message or \"rate limit\" in error_message.lower():\n",
    "            print(\"\\n‚ùå Rate Limit Error:\")\n",
    "            print(\"   ‚Üí You've exceeded the API rate limit\")\n",
    "            print(\"   ‚Üí Consider implementing retry logic with exponential backoff\")\n",
    "            print(\"   ‚Üí Reference notebook 107 for retry patterns\")\n",
    "            \n",
    "            state[\"messages\"].append(AIMessage(\n",
    "                content=\"[Rate limit exceeded - implement retry logic]\"\n",
    "            ))\n",
    "            \n",
    "        elif \"500\" in error_message or \"502\" in error_message or \"503\" in error_message:\n",
    "            print(\"\\n‚ùå Server Error:\")\n",
    "            print(f\"   ‚Üí API server error: {error_message}\")\n",
    "            print(\"   ‚Üí This is temporary - retry after a brief delay\")\n",
    "            \n",
    "            state[\"messages\"].append(AIMessage(\n",
    "                content=\"[Server error - retry after delay]\"\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            # Unknown error\n",
    "            print(f\"\\n‚ùå Unexpected Error: {error_message}\")\n",
    "            print(\"   ‚Üí Review the error message and check API documentation\")\n",
    "            \n",
    "            state[\"messages\"].append(AIMessage(\n",
    "                content=f\"[Error: {error_message[:100]}]\"\n",
    "            ))\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Production error handling pattern defined!\")\n",
    "print(\"\\nüí° This pattern handles:\")\n",
    "print(\"   ‚Ä¢ ConnectionError - Network issues\")\n",
    "print(\"   ‚Ä¢ TimeoutError - API timeouts\")\n",
    "print(\"   ‚Ä¢ HTTP 401 - Authentication failures\")\n",
    "print(\"   ‚Ä¢ HTTP 429 - Rate limit exceeded\")\n",
    "print(\"   ‚Ä¢ HTTP 500/502/503 - Server errors\")\n",
    "print(\"\\n‚≠ê Key insight:\")\n",
    "print(\"   Even when LLM calls fail, we preserve conversation state\")\n",
    "print(\"   and append error messages for debugging context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416yc28h1bq",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.680548Z",
     "iopub.status.busy": "2025-10-22T09:54:40.680482Z",
     "iopub.status.idle": "2025-10-22T09:54:40.758789Z",
     "shell.execute_reply": "2025-10-22T09:54:40.758252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "\n",
    "print(\"\\nüìä Graph visualization above shows:\")\n",
    "print(\"  - Single node: process_query\")\n",
    "print(\"  - Linear flow: START ‚Üí process_query ‚Üí END\")\n",
    "print(\"  - Memory happens INSIDE the node via state mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039ub13wto",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Implementing the Conversation Loop\n",
    "\n",
    "### The Challenge: Synchronizing History\n",
    "\n",
    "We have a problem to solve: **how do we maintain conversation history across multiple invocations?**\n",
    "\n",
    "**The issue:**\n",
    "- Each time we call `agent.invoke()`, we pass initial state\n",
    "- But we need to PERSIST the conversation between calls\n",
    "- The graph doesn't automatically remember between invocations (yet!)\n",
    "\n",
    "**The solution:** Maintain an external `conversation_history` variable and synchronize it after each turn.\n",
    "\n",
    "### The Pattern\n",
    "\n",
    "```python\n",
    "# External history variable\n",
    "conversation_history = []\n",
    "\n",
    "# Turn 1\n",
    "conversation_history.append(HumanMessage(content=\"Turn 1 query\"))\n",
    "result = agent.invoke({\"messages\": conversation_history})\n",
    "conversation_history = result[\"messages\"]  # ‚≠ê Sync!\n",
    "\n",
    "# Turn 2\n",
    "conversation_history.append(HumanMessage(content=\"Turn 2 query\"))\n",
    "result = agent.invoke({\"messages\": conversation_history})\n",
    "conversation_history = result[\"messages\"]  # ‚≠ê Sync again!\n",
    "```\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Before invoke**: We add the new human message to our history\n",
    "2. **During invoke**: The agent processes ALL messages and appends AI response\n",
    "3. **After invoke**: We update our history with the full conversation (including AI response)\n",
    "4. **Next turn**: We start with the complete history\n",
    "\n",
    "This manual synchronization ensures context is preserved across invocations.\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uovyyofyvzs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.760124Z",
     "iopub.status.busy": "2025-10-22T09:54:40.760025Z",
     "iopub.status.idle": "2025-10-22T09:54:40.762485Z",
     "shell.execute_reply": "2025-10-22T09:54:40.762174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def chat(user_message: str):\n",
    "    \"\"\"\n",
    "    Handle a single conversation turn.\n",
    "    \n",
    "    Args:\n",
    "        user_message: The user's input\n",
    "    \n",
    "    Returns:\n",
    "        The AI's response content\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    # 1. Add human message to history\n",
    "    conversation_history.append(HumanMessage(content=user_message))\n",
    "    \n",
    "    print(f\"üë§ You: {user_message}\")\n",
    "    \n",
    "    # 2. Invoke agent with full history\n",
    "    result = agent.invoke({\"messages\": conversation_history})\n",
    "    \n",
    "    # 3. ‚≠ê CRITICAL: Sync history with result\n",
    "    conversation_history = result[\"messages\"]\n",
    "    \n",
    "    # 4. Return the last AI message\n",
    "    return conversation_history[-1].content\n",
    "\n",
    "print(\"‚úÖ Conversation loop function defined!\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print('   chat(\"My firewall is fw-prod-01\")')\n",
    "print('   chat(\"What is my firewall hostname?\")')\n",
    "print(\"\\n‚≠ê Key insight:\")\n",
    "print(\"   conversation_history = result['messages'] synchronizes state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fsfezss4cr",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Challenges of Manual Memory Management\n",
    "\n",
    "Before we test our implementation, let's be explicit about the **pain points** of this manual approach:\n",
    "\n",
    "**1. Verbose and Repetitive**\n",
    "```python\n",
    "# This pattern must be repeated for EVERY chat interaction:\n",
    "conversation_history.append(HumanMessage(content=user_message))\n",
    "result = agent.invoke({\"messages\": conversation_history})\n",
    "conversation_history = result[\"messages\"]  # Don't forget this!\n",
    "```\n",
    "\n",
    "**2. Error-Prone**\n",
    "```python\n",
    "# Forget ONE sync and you lose context:\n",
    "conversation_history.append(HumanMessage(content=\"Question\"))\n",
    "result = agent.invoke({\"messages\": conversation_history})\n",
    "# ‚ùå OOPS! Forgot to sync - next turn won't have AI response!\n",
    "```\n",
    "\n",
    "**3. Manual State Synchronization**\n",
    "```python\n",
    "# You're responsible for keeping external state in sync:\n",
    "state[\"messages\"].append(AIMessage(...))  # Inside node\n",
    "conversation_history = result[\"messages\"]  # Outside node\n",
    "# Must happen EVERY time, no exceptions!\n",
    "```\n",
    "\n",
    "**4. No Automatic Merging**\n",
    "```python\n",
    "# With reducers (notebook 110), this happens automatically:\n",
    "# state[\"messages\"].append(msg)  # ‚úÖ Automatic merge!\n",
    "\n",
    "# Without reducers (this notebook), you must:\n",
    "# - Manually append human messages\n",
    "# - Manually sync after invoke\n",
    "# - Manually handle state updates\n",
    "```\n",
    "\n",
    "**5. Boilerplate Everywhere**\n",
    "Every conversation function needs the same pattern:\n",
    "- Add human message\n",
    "- Invoke agent\n",
    "- Sync history\n",
    "- Extract response\n",
    "\n",
    "**6. Token Cost Awareness Required**\n",
    "You must manually implement trimming, summarization, or windowing to control costs.\n",
    "\n",
    "### Why Learn This Manual Approach?\n",
    "\n",
    "If it's so painful, why teach it? **Because understanding manual management:**\n",
    "\n",
    "‚úÖ Shows you HOW conversation memory works under the hood  \n",
    "‚úÖ Helps you debug when automatic solutions fail  \n",
    "‚úÖ Makes you appreciate the value of reducers (notebook 110)  \n",
    "‚úÖ Gives you full control when you need custom behavior  \n",
    "\n",
    "**The good news:** Notebook 110 introduces `add_messages` reducer that eliminates ALL of this manual work while giving you MORE control!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u82s6tkgbte",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üö® What Happens When Manual Sync Fails?\n",
    "\n",
    "Let's demonstrate the **error-prone nature** of manual memory management by intentionally forgetting to sync the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mb5hjar2wyp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:40.763940Z",
     "iopub.status.busy": "2025-10-22T09:54:40.763840Z",
     "iopub.status.idle": "2025-10-22T09:54:43.911474Z",
     "shell.execute_reply": "2025-10-22T09:54:43.910699Z"
    }
   },
   "outputs": [],
   "source": [
    "def chat_broken(user_message: str):\n",
    "    \"\"\"\n",
    "    ‚ùå BROKEN: Chat function that FORGETS to sync history.\n",
    "    \n",
    "    This demonstrates what happens when you forget the critical sync step.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    # 1. Add human message\n",
    "    conversation_history.append(HumanMessage(content=user_message))\n",
    "    print(f\"üë§ You: {user_message}\")\n",
    "    \n",
    "    # 2. Invoke agent\n",
    "    result = agent.invoke({\"messages\": conversation_history})\n",
    "    \n",
    "    # 3. ‚ùå BUG: Forgot to sync history!\n",
    "    # conversation_history = result[\"messages\"]  # <-- MISSING!\n",
    "    \n",
    "    # The AI's response is lost from conversation_history!\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "# Reset conversation\n",
    "conversation_history = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMONSTRATION: Manual Sync Failure\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è  Using BROKEN chat function that forgets to sync history\\n\")\n",
    "\n",
    "# Turn 1\n",
    "print(\"‚îÄ\"*70)\n",
    "print(\"TURN 1: Provide firewall hostname\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat_broken(\"My firewall hostname is fw-datacenter-01\")\n",
    "print(f\"üìä History length after turn 1: {len(conversation_history)} messages\")\n",
    "print(f\"   Contents: {[type(m).__name__ for m in conversation_history]}\")\n",
    "\n",
    "# Turn 2\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 2: Ask about the hostname (should remember it!)\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat_broken(\"What is my firewall hostname?\")\n",
    "print(f\"üìä History length after turn 2: {len(conversation_history)} messages\")\n",
    "print(f\"   Contents: {[type(m).__name__ for m in conversation_history]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üö® PROBLEM IDENTIFIED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ùå What went wrong:\")\n",
    "print(\"   ‚Ä¢ History only has HumanMessages - no AIMessages!\")\n",
    "print(\"   ‚Ä¢ The bot's responses were NEVER saved to history\")\n",
    "print(\"   ‚Ä¢ Each turn is isolated - no memory of AI responses\")\n",
    "print(\"   ‚Ä¢ The bot CAN'T reference its own previous answers\")\n",
    "print(\"\\nüí° Expected history: [HumanMessage, AIMessage, HumanMessage, AIMessage]\")\n",
    "print(f\"   Actual history:   {[type(m).__name__ for m in conversation_history]}\")\n",
    "print(\"\\n‚ö†Ô∏è  This is why the sync step is CRITICAL:\")\n",
    "print(\"   conversation_history = result['messages']  # Don't forget!\")\n",
    "print(\"\\n‚úÖ The correct chat() function (defined earlier) handles this properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m7ou8odg32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Testing with Memory\n",
    "\n",
    "### The Moment of Truth\n",
    "\n",
    "Now let's test if our bot can remember! We'll have a multi-turn conversation about PAN-OS upgrades.\n",
    "\n",
    "**Test scenario:**\n",
    "1. Tell the bot our current version\n",
    "2. Ask about upgrade paths (should remember the version!)\n",
    "3. Ask follow-up questions\n",
    "\n",
    "### What to Watch For\n",
    "\n",
    "- **Turn 1**: Bot acknowledges information\n",
    "- **Turn 2**: Bot references Turn 1 information (MEMORY!)\n",
    "- **Turn 3**: Bot maintains full context\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bl0vs4haab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:43.913388Z",
     "iopub.status.busy": "2025-10-22T09:54:43.913242Z",
     "iopub.status.idle": "2025-10-22T09:54:50.604978Z",
     "shell.execute_reply": "2025-10-22T09:54:50.603923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Turn 1: Provide context\n",
    "print(\"=\"*60)\n",
    "print(\"TURN 1: Establishing context\")\n",
    "print(\"=\"*60)\n",
    "chat(\"My firewall is running PAN-OS 10.1.0 and the hostname is fw-prod-01\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TURN 2: Testing memory - asking about upgrade path\")\n",
    "print(\"=\"*60)\n",
    "chat(\"What's the recommended upgrade path for my firewall?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TURN 3: Testing continued memory\")\n",
    "print(\"=\"*60)\n",
    "chat(\"What was my firewall's hostname again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dlvjsp0bl5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:50.607711Z",
     "iopub.status.busy": "2025-10-22T09:54:50.607498Z",
     "iopub.status.idle": "2025-10-22T09:54:50.612272Z",
     "shell.execute_reply": "2025-10-22T09:54:50.611699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the conversation history\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERSATION HISTORY DEBUG\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal messages: {len(conversation_history)}\")\n",
    "print(\"\\nFull conversation:\")\n",
    "for i, msg in enumerate(conversation_history, 1):\n",
    "    speaker = \"üë§ Human\" if isinstance(msg, HumanMessage) else \"ü§ñ AI\"\n",
    "    content_preview = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n",
    "    print(f\"\\n{i}. {speaker}\")\n",
    "    print(f\"   {content_preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ SUCCESS! The bot remembers context across turns!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Key observations:\")\n",
    "print(\"   - Turn 2: Bot referenced PAN-OS 10.1.0 from Turn 1\")\n",
    "print(\"   - Turn 3: Bot recalled fw-prod-01 from Turn 1\")\n",
    "print(\"   - History grows: Human ‚Üí AI ‚Üí Human ‚Üí AI ‚Üí Human ‚Üí AI\")\n",
    "print(\"\\n‚≠ê This is REAL conversation memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jbryhbjnnp",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.5 Practical Example: SCM NAT Policy Wizard\n",
    "\n",
    "### Real-World Use Case\n",
    "\n",
    "Let's build a practical **multi-turn wizard** for creating a NAT policy in Strata Cloud Manager. This demonstrates how conversation memory enables complex configuration workflows.\n",
    "\n",
    "**Scenario**: A network engineer needs to create a NAT policy but doesn't have all details upfront. The wizard collects information across multiple turns:\n",
    "\n",
    "1. **Turn 1**: Identify the need (NAT policy for web server)\n",
    "2. **Turn 2**: Collect zone information\n",
    "3. **Turn 3**: Gather address details  \n",
    "4. **Turn 4**: Summarize and confirm\n",
    "\n",
    "This mirrors real troubleshooting and configuration workflows where context builds over multiple interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85zhdq6wtdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:54:50.614315Z",
     "iopub.status.busy": "2025-10-22T09:54:50.614173Z",
     "iopub.status.idle": "2025-10-22T09:55:14.035645Z",
     "shell.execute_reply": "2025-10-22T09:55:14.034550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset conversation for clean demo\n",
    "conversation_history = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRACTICAL EXAMPLE: SCM NAT Policy Configuration Wizard\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüéØ Goal: Create a NAT policy through multi-turn conversation\")\n",
    "print(\"üí° Watch how the bot remembers context from each turn!\\n\")\n",
    "\n",
    "# Turn 1: Initial request\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 1: Engineer states the requirement\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"I need to create a NAT policy for my web server that needs to be accessible from the internet\")\n",
    "\n",
    "# Turn 2: Provide zone details\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 2: Provide zone information\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"The source zone is 'untrust' for internet traffic, and destination zone is 'dmz' where the web server lives\")\n",
    "\n",
    "# Turn 3: Provide address details\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 3: Provide addressing information\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"The web server's internal IP is 10.50.100.10 and it should be NATted to public IP 203.0.113.50 on port 443\")\n",
    "\n",
    "# Turn 4: Request summary\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 4: Request configuration summary\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"Can you summarize the complete NAT policy configuration we just defined?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ WIZARD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Turn 1: Bot understood the general requirement\")\n",
    "print(\"   ‚Ä¢ Turn 2: Bot remembered it was about NAT and added zone context\")\n",
    "print(\"   ‚Ä¢ Turn 3: Bot retained zones AND added address details\")\n",
    "print(\"   ‚Ä¢ Turn 4: Bot recalled ALL information to create complete summary\")\n",
    "print(\"\\n‚≠ê This is the power of conversation memory for complex workflows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9sxiv1qv",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.6 Another Practical Example: Address Object Creation Wizard\n",
    "\n",
    "Let's build another real-world wizard - this time for creating address objects in SCM. This demonstrates how memory enables incremental data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4lx1c7dvb07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:55:14.039099Z",
     "iopub.status.busy": "2025-10-22T09:55:14.038757Z",
     "iopub.status.idle": "2025-10-22T09:55:32.127169Z",
     "shell.execute_reply": "2025-10-22T09:55:32.126100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset for new wizard\n",
    "conversation_history = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRACTICAL EXAMPLE: SCM Address Object Creation Wizard\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüéØ Goal: Create address objects with incremental information gathering\")\n",
    "print(\"üí° Demonstrating multi-turn data collection workflow\\n\")\n",
    "\n",
    "# Turn 1: Start the workflow\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 1: Initiate address object creation\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"I need to create address objects for my new branch office network\")\n",
    "\n",
    "# Turn 2: Specify network details\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 2: Provide network information\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"The branch office uses 10.20.0.0/16 network space\")\n",
    "\n",
    "# Turn 3: Add specific subnets\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 3: Define subnet breakdown\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"We need separate objects for users (10.20.10.0/24), servers (10.20.20.0/24), and guest wifi (10.20.30.0/24)\")\n",
    "\n",
    "# Turn 4: Add naming convention\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 4: Specify naming convention\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"Use the naming pattern: branch-office-seattle-<subnet-type>\")\n",
    "\n",
    "# Turn 5: Request configuration summary\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 5: Generate complete configuration\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"Can you list all the address objects we need to create with their names and IP ranges?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ADDRESS OBJECT WIZARD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìä Conversation Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total turns: {len([m for m in conversation_history if isinstance(m, HumanMessage)])}\")\n",
    "print(f\"   ‚Ä¢ Total messages: {len(conversation_history)}\")\n",
    "print(f\"   ‚Ä¢ Information gathered across: 5 separate interactions\")\n",
    "print(\"\\nüí° Notice how the bot:\")\n",
    "print(\"   ‚Ä¢ Remembered the branch office context from turn 1\")\n",
    "print(\"   ‚Ä¢ Retained the network space from turn 2\")\n",
    "print(\"   ‚Ä¢ Recalled all three subnets from turn 3\")\n",
    "print(\"   ‚Ä¢ Applied the naming convention from turn 4\")\n",
    "print(\"   ‚Ä¢ Synthesized everything into a complete config in turn 5\")\n",
    "print(\"\\n‚≠ê This incremental data collection is impossible without memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hautalh4t8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.7 Third Practical Example: Security Rule Configuration Wizard\n",
    "\n",
    "Let's add one more real-world wizard - this time for creating security rules in SCM. This demonstrates how conversation memory handles complex multi-parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0whiklowpjip",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:55:32.131496Z",
     "iopub.status.busy": "2025-10-22T09:55:32.131127Z",
     "iopub.status.idle": "2025-10-22T09:55:54.553075Z",
     "shell.execute_reply": "2025-10-22T09:55:54.551822Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset for security rule wizard\n",
    "conversation_history = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRACTICAL EXAMPLE: SCM Security Rule Configuration Wizard\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüéØ Goal: Create security rule with incremental parameter gathering\")\n",
    "print(\"üí° Security rules have many parameters - perfect for multi-turn wizards\\n\")\n",
    "\n",
    "# Turn 1: Initial requirement\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 1: State the requirement\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"I need to create a security rule to allow HTTPS access to our web servers in the DMZ from the internet\")\n",
    "\n",
    "# Turn 2: Specify zones\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 2: Define security zones\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"The traffic will go from 'untrust' zone (source) to 'dmz' zone (destination)\")\n",
    "\n",
    "# Turn 3: Define source/destination\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 3: Specify source and destination details\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"Source should be 'any' since it's from the internet, and destination should be address group 'web-servers-dmz'\")\n",
    "\n",
    "# Turn 4: Application and service\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 4: Application and service details\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"For application use 'ssl' and 'web-browsing', service should be 'application-default'\")\n",
    "\n",
    "# Turn 5: Security profile and action\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 5: Security settings\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"Set action to 'allow', attach 'strict-security' profile group, and enable logging at session end\")\n",
    "\n",
    "# Turn 6: Request complete configuration\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"TURN 6: Generate complete security rule configuration\")\n",
    "print(\"‚îÄ\"*70)\n",
    "chat(\"Can you provide a complete summary of the security rule we just designed, including all parameters and security best practices we should consider?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ SECURITY RULE WIZARD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìä Conversation Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total turns: {len([m for m in conversation_history if isinstance(m, HumanMessage)])}\")\n",
    "print(f\"   ‚Ä¢ Total messages: {len(conversation_history)}\")\n",
    "print(f\"   ‚Ä¢ Rule parameters gathered: 10+ across 6 interactions\")\n",
    "print(\"\\nüí° Notice the complexity:\")\n",
    "print(\"   ‚Ä¢ Turn 1: Identified requirement (HTTPS to DMZ)\")\n",
    "print(\"   ‚Ä¢ Turn 2: Remembered requirement + added zones\")\n",
    "print(\"   ‚Ä¢ Turn 3: Retained zones + added source/destination\")\n",
    "print(\"   ‚Ä¢ Turn 4: Kept all context + added application/service\")\n",
    "print(\"   ‚Ä¢ Turn 5: Preserved all parameters + added security settings\")\n",
    "print(\"   ‚Ä¢ Turn 6: Synthesized complete rule with best practices\")\n",
    "print(\"\\n‚≠ê Security rules have 15+ parameters - memory makes complex configs manageable!\")\n",
    "print(\"\\nüîê This pattern applies to:\")\n",
    "print(\"   ‚Ä¢ Security policies (shown here)\")\n",
    "print(\"   ‚Ä¢ NAT rules (Section 7.5)\")\n",
    "print(\"   ‚Ä¢ QoS policies\")\n",
    "print(\"   ‚Ä¢ VPN configurations\")\n",
    "print(\"   ‚Ä¢ Any complex multi-parameter SCM objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2apl5q0sk4x",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Problem 1: Conversation Persistence\n",
    "\n",
    "### The Issue\n",
    "\n",
    "Our conversation memory works... **but only while the notebook is running!**\n",
    "\n",
    "**What happens when:**\n",
    "- The notebook kernel restarts?\n",
    "- The user closes their session?\n",
    "- The application crashes?\n",
    "\n",
    "**Answer:** All conversation history is LOST because it's stored in a Python variable in memory.\n",
    "\n",
    "### Real-World Scenarios\n",
    "\n",
    "In production, you need to persist conversations:\n",
    "\n",
    "1. **Customer Support Bot**: User comes back tomorrow and expects the bot to remember previous issues\n",
    "2. **Network Operations Assistant**: Shift handoffs require preserving troubleshooting context\n",
    "3. **Configuration Wizard**: Multi-session workflows need to resume where they left off\n",
    "\n",
    "### Solution Approaches\n",
    "\n",
    "**Option 1: JSON File Storage**\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Save conversation\n",
    "with open('conversation.json', 'w') as f:\n",
    "    json.dump([msg.dict() for msg in conversation_history], f)\n",
    "\n",
    "# Load conversation\n",
    "with open('conversation.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    conversation_history = [HumanMessage(**m) if m['type']=='human' \n",
    "                           else AIMessage(**m) for m in data]\n",
    "```\n",
    "\n",
    "**Option 2: Database Storage (PostgreSQL, MongoDB)**\n",
    "```python\n",
    "# Pseudocode\n",
    "db.conversations.insert({\n",
    "    'session_id': 'user-123',\n",
    "    'messages': conversation_history,\n",
    "    'timestamp': datetime.now()\n",
    "})\n",
    "```\n",
    "\n",
    "**Option 3: LangGraph Checkpointing** ‚≠ê (Coming in notebook 110!)\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "agent = graph.compile(checkpointer=checkpointer)\n",
    "# Automatically persists state!\n",
    "```\n",
    "\n",
    "### Why We Haven't Implemented This Yet\n",
    "\n",
    "For this notebook, we're focusing on **understanding manual memory management**. Persistence adds complexity, and we want you to grasp the fundamentals first.\n",
    "\n",
    "**Next notebook (110):** We'll introduce **reducers and tools** which handle memory management automatically!\n",
    "\n",
    "### Quick Demo: Saving to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cwtt2l141ep",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:55:54.556329Z",
     "iopub.status.busy": "2025-10-22T09:55:54.556118Z",
     "iopub.status.idle": "2025-10-22T09:55:54.562236Z",
     "shell.execute_reply": "2025-10-22T09:55:54.561570Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_conversation(filename: str = \"conversation.json\"):\n",
    "    \"\"\"Save conversation history to JSON file.\"\"\"\n",
    "    # Convert messages to dict format\n",
    "    messages_data = []\n",
    "    for msg in conversation_history:\n",
    "        messages_data.append({\n",
    "            'type': 'human' if isinstance(msg, HumanMessage) else 'ai',\n",
    "            'content': msg.content\n",
    "        })\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(messages_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(messages_data)} messages to {filename}\")\n",
    "\n",
    "def load_conversation(filename: str = \"conversation.json\"):\n",
    "    \"\"\"Load conversation history from JSON file.\"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        messages_data = json.load(f)\n",
    "    \n",
    "    # Convert back to message objects\n",
    "    conversation_history = []\n",
    "    for msg_data in messages_data:\n",
    "        if msg_data['type'] == 'human':\n",
    "            conversation_history.append(HumanMessage(content=msg_data['content']))\n",
    "        else:\n",
    "            conversation_history.append(AIMessage(content=msg_data['content']))\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(conversation_history)} messages from {filename}\")\n",
    "\n",
    "# Demo: Save current conversation\n",
    "save_conversation()\n",
    "\n",
    "print(\"\\nüí° Now if the kernel restarts, you can:\")\n",
    "print('   load_conversation()')\n",
    "print(\"   # Resume conversation with full history!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ucvtmtj3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Problem 2: Growing Token Costs\n",
    "\n",
    "### The Issue\n",
    "\n",
    "Every time we invoke the LLM, we send the **ENTIRE conversation history**. This works great for memory, but creates a problem:\n",
    "\n",
    "**Token costs grow exponentially with each turn!**\n",
    "\n",
    "### Cost Illustration\n",
    "\n",
    "Assume each message averages 50 tokens:\n",
    "\n",
    "| Turn | Messages Sent | Total Tokens | Cost Multiplier |\n",
    "|------|---------------|--------------|-----------------|\n",
    "| 1    | 1             | 50           | 1x              |\n",
    "| 2    | 3             | 150          | 3x              |\n",
    "| 3    | 5             | 250          | 5x              |\n",
    "| 4    | 7             | 350          | 7x              |\n",
    "| 10   | 19            | 950          | 19x             |\n",
    "| 50   | 99            | 4,950        | 99x             |\n",
    "\n",
    "**After 50 turns**: You're paying 99x more per query than turn 1!\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "**Example pricing (GPT-4):**\n",
    "- Input: $0.03 / 1K tokens\n",
    "- Turn 1: $0.0015\n",
    "- Turn 50: $0.15 (100x more!)\n",
    "- 1000 users √ó 50 turns = **$150,000** üò±\n",
    "\n",
    "### Why This Happens\n",
    "\n",
    "```python\n",
    "# Turn 1: Send 1 message\n",
    "llm.invoke([HumanMessage(\"Hello\")])  # 1 message\n",
    "\n",
    "# Turn 2: Send 3 messages (H, A, H)\n",
    "llm.invoke([HumanMessage(\"Hello\"), \n",
    "           AIMessage(\"Hi!\"), \n",
    "           HumanMessage(\"How are you?\")])  # 3 messages\n",
    "\n",
    "# Turn 3: Send 5 messages (H, A, H, A, H)\n",
    "llm.invoke([...all previous messages...])  # 5 messages\n",
    "```\n",
    "\n",
    "Each turn re-sends ALL previous messages!\n",
    "\n",
    "### Solution Strategies\n",
    "\n",
    "**1. Message Window Trimming** (Keep last N messages)\n",
    "```python\n",
    "MAX_HISTORY = 10\n",
    "recent_messages = conversation_history[-MAX_HISTORY:]\n",
    "result = agent.invoke({\"messages\": recent_messages})\n",
    "```\n",
    "\n",
    "**2. Summarization** (Compress old messages)\n",
    "```python\n",
    "# Every 20 messages, summarize the conversation\n",
    "if len(conversation_history) > 20:\n",
    "    summary = summarize_conversation(conversation_history[:20])\n",
    "    conversation_history = [HumanMessage(content=f\"Summary: {summary}\")] + conversation_history[20:]\n",
    "```\n",
    "\n",
    "**3. Smart Truncation** (Keep system messages + recent)\n",
    "```python\n",
    "system_messages = [msg for msg in conversation_history if isinstance(msg, SystemMessage)]\n",
    "recent_messages = conversation_history[-10:]\n",
    "trimmed = system_messages + recent_messages\n",
    "```\n",
    "\n",
    "**4. Token-Based Trimming** (Stay under budget)\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "def trim_to_token_limit(messages, max_tokens=4000):\n",
    "    encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    # Count tokens and trim from the start\n",
    "    # (implementation left as exercise)\n",
    "```\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Window Trimming | Simple, predictable | Loses old context |\n",
    "| Summarization | Preserves key info | Adds LLM calls |\n",
    "| Smart Truncation | Keeps important messages | Complex logic |\n",
    "| Token-Based | Exact cost control | Requires token counting |\n",
    "\n",
    "### Demo: Message Window Trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n6jbf07yn3s",
   "metadata": {},
   "source": [
    "### Real Token Counting with tiktoken\n",
    "\n",
    "Let's move beyond theoretical costs and actually count tokens in our conversations using `tiktoken`, OpenAI's tokenizer library (also useful for Claude token estimation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pl9y56u2m1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:55:54.564142Z",
     "iopub.status.busy": "2025-10-22T09:55:54.564001Z",
     "iopub.status.idle": "2025-10-22T09:55:54.571974Z",
     "shell.execute_reply": "2025-10-22T09:55:54.571511Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    tiktoken_available = True\n",
    "except ImportError:\n",
    "    tiktoken_available = False\n",
    "    print(\"‚ö†Ô∏è  tiktoken not installed - run: pip install tiktoken\")\n",
    "    print(\"   Proceeding with character-based estimation...\")\n",
    "\n",
    "def count_tokens(messages: list, model: str = \"gpt-4\") -> dict:\n",
    "    \"\"\"\n",
    "    Count tokens in a message list.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of HumanMessage/AIMessage objects\n",
    "        model: Model name for tokenizer (defaults to gpt-4)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with token counts and statistics\n",
    "    \"\"\"\n",
    "    if tiktoken_available:\n",
    "        # Use actual tokenizer\n",
    "        encoder = tiktoken.encoding_for_model(model)\n",
    "        \n",
    "        total_tokens = 0\n",
    "        message_tokens = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            tokens = len(encoder.encode(msg.content))\n",
    "            message_tokens.append(tokens)\n",
    "            total_tokens += tokens\n",
    "        \n",
    "        return {\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"message_tokens\": message_tokens,\n",
    "            \"avg_tokens_per_message\": total_tokens / len(messages) if messages else 0,\n",
    "            \"method\": \"tiktoken (accurate)\"\n",
    "        }\n",
    "    else:\n",
    "        # Fallback to character estimation\n",
    "        # Rule of thumb: ~4 characters per token for English text\n",
    "        total_chars = sum(len(msg.content) for msg in messages)\n",
    "        estimated_tokens = total_chars // 4\n",
    "        \n",
    "        return {\n",
    "            \"total_tokens\": estimated_tokens,\n",
    "            \"message_tokens\": [len(msg.content) // 4 for msg in messages],\n",
    "            \"avg_tokens_per_message\": estimated_tokens / len(messages) if messages else 0,\n",
    "            \"method\": \"character estimation (~4 chars/token)\"\n",
    "        }\n",
    "\n",
    "def analyze_conversation_costs(messages: list, model: str = \"gpt-4\") -> None:\n",
    "    \"\"\"\n",
    "    Analyze and display conversation token costs.\n",
    "    \n",
    "    Args:\n",
    "        messages: Conversation history\n",
    "        model: Model name for cost calculation\n",
    "    \"\"\"\n",
    "    stats = count_tokens(messages, model)\n",
    "    \n",
    "    # Cost per 1K tokens (example rates - check current pricing!)\n",
    "    cost_per_1k_input = 0.03  # GPT-4 input pricing\n",
    "    cost_per_1k_output = 0.06  # GPT-4 output pricing\n",
    "    \n",
    "    # Calculate costs (assuming ~50/50 input/output split)\n",
    "    total_cost = (stats[\"total_tokens\"] / 1000) * cost_per_1k_input\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TOKEN ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"   Total messages: {len(messages)}\")\n",
    "    print(f\"   Total tokens: {stats['total_tokens']:,}\")\n",
    "    print(f\"   Avg tokens/message: {stats['avg_tokens_per_message']:.1f}\")\n",
    "    print(f\"   Method: {stats['method']}\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Cost Estimate (at ${cost_per_1k_input}/1K input tokens):\")\n",
    "    print(f\"   This conversation: ${total_cost:.4f}\")\n",
    "    print(f\"   Per turn: ${total_cost / (len(messages) / 2):.4f}\")\n",
    "    \n",
    "    # Project costs over time\n",
    "    print(f\"\\nüìà Cost Projection:\")\n",
    "    print(f\"   After 10 turns: ${total_cost * (10 / (len(messages) / 2)):.4f}\")\n",
    "    print(f\"   After 50 turns: ${total_cost * (50 / (len(messages) / 2)):.4f}\")\n",
    "    print(f\"   After 100 turns: ${total_cost * (100 / (len(messages) / 2)):.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Token growth:\")\n",
    "    print(f\"   Turn 1: ~{stats['message_tokens'][0] if stats['message_tokens'] else 0} tokens\")\n",
    "    print(f\"   Turn {len(messages) // 2}: ~{stats['total_tokens']:,} tokens\")\n",
    "    print(f\"   Growth rate: {(stats['total_tokens'] / stats['message_tokens'][0]):.1f}x\" if stats['message_tokens'] and stats['message_tokens'][0] > 0 else \"   Growth rate: N/A\")\n",
    "\n",
    "# Demo with our current conversation history\n",
    "if len(conversation_history) > 0:\n",
    "    analyze_conversation_costs(conversation_history)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üí° KEY INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n1. Token counts grow LINEARLY with conversation length\")\n",
    "    print(\"   Each turn adds both human + AI message tokens\")\n",
    "    print(\"\\n2. Costs grow QUADRATICALLY if you send full history each time\")\n",
    "    print(\"   Turn 1: 1 message, Turn 2: 3 messages, Turn 3: 5 messages...\")\n",
    "    print(\"\\n3. Token trimming controls costs:\")\n",
    "    print(\"   Keep last 10 messages: ~constant cost per turn\")\n",
    "    print(\"   Send full history: costs increase every turn\")\n",
    "    print(\"\\n4. For long conversations:\")\n",
    "    print(\"   ‚Ä¢ Use trimming (last N messages)\")\n",
    "    print(\"   ‚Ä¢ Use summarization (compress old context)\")\n",
    "    print(\"   ‚Ä¢ Use checkpointing with smart retrieval\")\n",
    "else:\n",
    "    print(\"No conversation history to analyze. Run one of the wizard examples first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nis8f7mpk1",
   "metadata": {},
   "source": [
    "### Conversation Analytics\n",
    "\n",
    "Beyond token counting, let's analyze conversation patterns, message lengths, and interaction statistics. This helps understand user behavior and optimize bot performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188mz6nuq3l",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:55:54.573453Z",
     "iopub.status.busy": "2025-10-22T09:55:54.573334Z",
     "iopub.status.idle": "2025-10-22T09:55:54.581981Z",
     "shell.execute_reply": "2025-10-22T09:55:54.581639Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def analyze_conversation(messages: list) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive conversation analytics.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analytics metrics\n",
    "    \"\"\"\n",
    "    if not messages:\n",
    "        return {\"error\": \"No messages to analyze\"}\n",
    "    \n",
    "    # Separate human and AI messages\n",
    "    human_messages = [m for m in messages if isinstance(m, HumanMessage)]\n",
    "    ai_messages = [m for m in messages if isinstance(m, AIMessage)]\n",
    "    \n",
    "    # Message length analysis\n",
    "    human_lengths = [len(m.content) for m in human_messages]\n",
    "    ai_lengths = [len(m.content) for m in ai_messages]\n",
    "    \n",
    "    # Word count analysis\n",
    "    human_words = [len(m.content.split()) for m in human_messages]\n",
    "    ai_words = [len(m.content.split()) for m in ai_messages]\n",
    "    \n",
    "    # Keyword extraction (simple approach)\n",
    "    all_text = \" \".join([m.content for m in messages])\n",
    "    # Remove common words and extract technical terms\n",
    "    words = re.findall(r'\\b[A-Za-z]{4,}\\b', all_text.lower())\n",
    "    common_words = {'this', 'that', 'with', 'from', 'have', 'what', 'when', \n",
    "                    'where', 'about', 'your', 'should', 'would', 'could'}\n",
    "    keywords = [w for w in words if w not in common_words]\n",
    "    keyword_counts = Counter(keywords).most_common(10)\n",
    "    \n",
    "    return {\n",
    "        \"total_turns\": len(human_messages),\n",
    "        \"total_messages\": len(messages),\n",
    "        \"human_messages\": len(human_messages),\n",
    "        \"ai_messages\": len(ai_messages),\n",
    "        \n",
    "        \"avg_human_length\": sum(human_lengths) / len(human_lengths) if human_lengths else 0,\n",
    "        \"avg_ai_length\": sum(ai_lengths) / len(ai_lengths) if ai_lengths else 0,\n",
    "        \"max_human_length\": max(human_lengths) if human_lengths else 0,\n",
    "        \"max_ai_length\": max(ai_lengths) if ai_lengths else 0,\n",
    "        \n",
    "        \"avg_human_words\": sum(human_words) / len(human_words) if human_words else 0,\n",
    "        \"avg_ai_words\": sum(ai_words) / len(ai_words) if ai_words else 0,\n",
    "        \n",
    "        \"top_keywords\": keyword_counts,\n",
    "        \n",
    "        \"conversation_ratio\": len(ai_messages) / len(human_messages) if human_messages else 0\n",
    "    }\n",
    "\n",
    "def display_conversation_analytics(messages: list) -> None:\n",
    "    \"\"\"\n",
    "    Display formatted conversation analytics.\n",
    "    \n",
    "    Args:\n",
    "        messages: Conversation history to analyze\n",
    "    \"\"\"\n",
    "    analytics = analyze_conversation(messages)\n",
    "    \n",
    "    if \"error\" in analytics:\n",
    "        print(f\"‚ùå {analytics['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CONVERSATION ANALYTICS REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìä Overview:\")\n",
    "    print(f\"   Total turns: {analytics['total_turns']}\")\n",
    "    print(f\"   Total messages: {analytics['total_messages']}\")\n",
    "    print(f\"   Human messages: {analytics['human_messages']}\")\n",
    "    print(f\"   AI messages: {analytics['ai_messages']}\")\n",
    "    print(f\"   AI/Human ratio: {analytics['conversation_ratio']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìù Message Length (characters):\")\n",
    "    print(f\"   Avg human message: {analytics['avg_human_length']:.0f}\")\n",
    "    print(f\"   Avg AI message: {analytics['avg_ai_length']:.0f}\")\n",
    "    print(f\"   Max human message: {analytics['max_human_length']}\")\n",
    "    print(f\"   Max AI message: {analytics['max_ai_length']}\")\n",
    "    print(f\"   AI response verbosity: {(analytics['avg_ai_length'] / analytics['avg_human_length']):.2f}x human input\" if analytics['avg_human_length'] > 0 else \"   AI response verbosity: N/A\")\n",
    "    \n",
    "    print(f\"\\nüí¨ Word Count:\")\n",
    "    print(f\"   Avg human words/message: {analytics['avg_human_words']:.1f}\")\n",
    "    print(f\"   Avg AI words/message: {analytics['avg_ai_words']:.1f}\")\n",
    "    \n",
    "    print(f\"\\nüîë Top Keywords:\")\n",
    "    for i, (keyword, count) in enumerate(analytics['top_keywords'][:5], 1):\n",
    "        print(f\"   {i}. '{keyword}': {count} occurrences\")\n",
    "    \n",
    "    print(f\"\\nüí° Insights:\")\n",
    "    \n",
    "    # Provide insights based on metrics\n",
    "    if analytics['avg_ai_length'] > analytics['avg_human_length'] * 3:\n",
    "        print(\"   ‚ö†Ô∏è  AI responses are very verbose (3x+ human input)\")\n",
    "        print(\"      Consider instructing the AI to be more concise\")\n",
    "    \n",
    "    if analytics['total_turns'] > 20:\n",
    "        print(f\"   ‚ö†Ô∏è  Long conversation ({analytics['total_turns']} turns)\")\n",
    "        print(\"      Consider implementing trimming or summarization\")\n",
    "    \n",
    "    if analytics['avg_human_words'] < 5:\n",
    "        print(\"   üí≠ Users are sending very short messages\")\n",
    "        print(\"      Bot may need to ask clarifying questions\")\n",
    "    \n",
    "    # Check for SCM-specific keywords\n",
    "    scm_keywords = {'firewall', 'rule', 'security', 'address', 'policy', 'zone', 'panos'}\n",
    "    found_scm = [kw for kw, _ in analytics['top_keywords'] if kw in scm_keywords]\n",
    "    if found_scm:\n",
    "        print(f\"   üîê SCM-focused conversation (keywords: {', '.join(found_scm)})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Demo with current conversation\n",
    "if len(conversation_history) > 0:\n",
    "    display_conversation_analytics(conversation_history)\n",
    "    \n",
    "    print(\"\\nüí° Use Cases for Analytics:\")\n",
    "    print(\"   ‚Ä¢ Optimize bot verbosity based on AI/human length ratios\")\n",
    "    print(\"   ‚Ä¢ Identify when to trigger trimming (turn count thresholds)\")\n",
    "    print(\"   ‚Ä¢ Detect conversation topics from keywords\")\n",
    "    print(\"   ‚Ä¢ Monitor user engagement (message length trends)\")\n",
    "    print(\"   ‚Ä¢ A/B test different bot personalities (compare analytics)\")\n",
    "else:\n",
    "    print(\"No conversation history to analyze. Run one of the wizard examples first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j5snvqfgzw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:55:54.583281Z",
     "iopub.status.busy": "2025-10-22T09:55:54.583205Z",
     "iopub.status.idle": "2025-10-22T09:55:54.586199Z",
     "shell.execute_reply": "2025-10-22T09:55:54.585824Z"
    }
   },
   "outputs": [],
   "source": [
    "def chat_with_trimming(user_message: str, max_history: int = 10):\n",
    "    \"\"\"\n",
    "    Chat with automatic history trimming.\n",
    "    \n",
    "    Args:\n",
    "        user_message: The user's input\n",
    "        max_history: Maximum number of messages to keep\n",
    "    \n",
    "    Returns:\n",
    "        The AI's response content\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    # 1. Add human message\n",
    "    conversation_history.append(HumanMessage(content=user_message))\n",
    "    \n",
    "    # 2. ‚≠ê Trim to last N messages\n",
    "    trimmed_history = conversation_history[-max_history:]\n",
    "    \n",
    "    print(f\"üë§ You: {user_message}\")\n",
    "    print(f\"üìä History: {len(conversation_history)} total, using last {len(trimmed_history)}\")\n",
    "    \n",
    "    # 3. Invoke with trimmed history\n",
    "    result = agent.invoke({\"messages\": trimmed_history})\n",
    "    \n",
    "    # 4. Important: Update full history (not trimmed)\n",
    "    conversation_history.append(AIMessage(content=result[\"messages\"][-1].content))\n",
    "    \n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "print(\"‚úÖ Trimming-enabled chat function defined!\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print('   chat_with_trimming(\"Question\", max_history=10)')\n",
    "print(\"\\n‚≠ê Benefits:\")\n",
    "print(\"   - Keeps full history for persistence\")\n",
    "print(\"   - Only sends recent messages to LLM\")\n",
    "print(\"   - Predictable token costs\")\n",
    "print(\"\\n‚ö†Ô∏è  Trade-off:\")\n",
    "print(\"   - LLM can't see messages beyond the window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rrfei9tjbo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:55:54.587502Z",
     "iopub.status.busy": "2025-10-22T09:55:54.587431Z",
     "iopub.status.idle": "2025-10-22T09:56:07.531889Z",
     "shell.execute_reply": "2025-10-22T09:56:07.530485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Simulate a long conversation\n",
    "print(\"=\"*60)\n",
    "print(\"SIMULATING LONG CONVERSATION WITH TRIMMING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset conversation\n",
    "conversation_history = []\n",
    "\n",
    "# Simulate 15 turns (will trigger trimming with max_history=10)\n",
    "for i in range(1, 6):\n",
    "    print(f\"\\n--- Turn {i} ---\")\n",
    "    chat_with_trimming(f\"Turn {i}: Tell me about PAN-OS feature #{i}\", max_history=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total messages in history: {len(conversation_history)}\")\n",
    "print(f\"Messages sent to LLM on last turn: 10 (max_history limit)\")\n",
    "print(f\"\\nüí° Without trimming, last turn would have sent {len(conversation_history)} messages!\")\n",
    "print(f\"   Token savings: {((len(conversation_history) - 10) / len(conversation_history) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me0hikzqoaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary\n",
    "\n",
    "Congratulations! You've built a conversational AI agent with memory. Let's recap what you've learned:\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **AIMessage and Union Types** - Tracking both sides of the conversation\n",
    "   - `Union[HumanMessage, AIMessage]` allows mixed message types\n",
    "   - Modern Python 3.10+ syntax: `list[HumanMessage | AIMessage]`\n",
    "   - Single list preserves conversation order\n",
    "   - Essential for conversation memory\n",
    "\n",
    "2. **Memory-Enabled State** - Extending state to support conversation\n",
    "   - Changed from `List[HumanMessage]` to `List[Union[HumanMessage, AIMessage]]`\n",
    "   - Single type change enables full conversation tracking\n",
    "   - State can now hold complete dialogue history\n",
    "\n",
    "3. **Conversation Node with Memory** - Implementing the memory mechanism\n",
    "   - `state[\"messages\"].append(AIMessage(...))` saves AI responses\n",
    "   - Each invoke builds on previous context\n",
    "   - LLM sees full conversation history\n",
    "\n",
    "4. **Manual History Management** - Synchronizing state across invocations\n",
    "   - External `conversation_history` variable\n",
    "   - `conversation_history = result[\"messages\"]` synchronizes state\n",
    "   - Pattern: add human message ‚Üí invoke ‚Üí sync history\n",
    "   - **Critical:** Forgetting sync breaks memory!\n",
    "\n",
    "5. **Real-World SCM Wizards** - Practical multi-turn workflows\n",
    "   - NAT policy configuration across 4 turns\n",
    "   - Address object creation with incremental data collection\n",
    "   - Demonstrates value of memory for complex operations\n",
    "   - Shows how context builds across interactions\n",
    "\n",
    "6. **Challenges of Manual Management** - Understanding the pain points\n",
    "   - Verbose and repetitive synchronization code\n",
    "   - Error-prone (forget sync = lose context)\n",
    "   - Manual state management is tedious\n",
    "   - No automatic message merging\n",
    "   - Boilerplate in every conversation function\n",
    "\n",
    "7. **Conversation Persistence** - Saving conversations beyond runtime\n",
    "   - JSON file storage for simple persistence\n",
    "   - Database options for production\n",
    "   - Checkpointing as the ultimate solution (next notebook!)\n",
    "\n",
    "8. **Token Cost Management** - Controlling growing costs\n",
    "   - Costs grow linearly with conversation length\n",
    "   - Window trimming keeps recent context\n",
    "   - Trade-off: cost savings vs. context loss\n",
    "\n",
    "### Why Manual Memory Management Matters\n",
    "\n",
    "Understanding this manual approach is crucial because:\n",
    "\n",
    "- **Fundamentals**: You now understand HOW conversation memory actually works\n",
    "- **Debugging**: When things go wrong, you can trace the message flow\n",
    "- **Appreciation**: You'll appreciate automated solutions (reducers, checkpointing) more\n",
    "- **Control**: You know when to use manual vs. automatic approaches\n",
    "\n",
    "### The Problems We Identified\n",
    "\n",
    "1. ‚ùå **Manual synchronization is tedious**: `conversation_history = result[\"messages\"]` every time\n",
    "2. ‚ùå **Error-prone**: Forget ONE sync and lose context completely\n",
    "3. ‚ùå **No automatic persistence**: Need custom code to save conversations\n",
    "4. ‚ùå **Token costs grow unbounded**: Need manual trimming logic\n",
    "5. ‚ùå **Boilerplate everywhere**: Lots of repetitive code\n",
    "6. ‚ùå **State management burden**: Developer responsible for all synchronization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next: The Better Way with Reducers and Tools\n",
    "\n",
    "In **notebook 110**, we'll solve ALL of these problems with LangGraph's powerful features:\n",
    "\n",
    "### 1. Automatic Message Merging with Reducers\n",
    "\n",
    "**Manual approach (this notebook):**\n",
    "```python\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Union[HumanMessage, AIMessage]]\n",
    "\n",
    "# Must manually sync every time:\n",
    "conversation_history.append(HumanMessage(content=msg))\n",
    "result = agent.invoke({\"messages\": conversation_history})\n",
    "conversation_history = result[\"messages\"]  # Don't forget!\n",
    "```\n",
    "\n",
    "**Automatic approach (notebook 110):**\n",
    "```python\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # ‚≠ê Auto-merge!\n",
    "\n",
    "# Just invoke - merging happens automatically:\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=msg)]})\n",
    "# No manual sync needed! The reducer handles everything!\n",
    "```\n",
    "\n",
    "### 2. Tool Calling - Agents That Take Actions\n",
    "\n",
    "**Problem:** Our current bot can only TALK about configurations - it can't actually DO anything.\n",
    "\n",
    "**Solution (notebook 110):**\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_firewall_version(hostname: str) -> str:\n",
    "    \"\"\"Get the PAN-OS version for a firewall.\"\"\"\n",
    "    # Could call SCM API here!\n",
    "    return \"10.1.0\"\n",
    "\n",
    "@tool\n",
    "def create_address_object(name: str, ip: str, folder: str) -> str:\n",
    "    \"\"\"Create an address object in SCM.\"\"\"\n",
    "    # Could use pan-scm-sdk here!\n",
    "    return f\"Created {name} with IP {ip}\"\n",
    "\n",
    "# Agent can now:\n",
    "# - Decide which tool to use\n",
    "# - Call tools with correct parameters\n",
    "# - Reason about tool results\n",
    "# - Take actual configuration actions\n",
    "```\n",
    "\n",
    "### 3. ReAct Pattern - Reasoning and Acting\n",
    "\n",
    "**Notebook 109:** Linear execution (START ‚Üí process ‚Üí END)\n",
    "\n",
    "**Notebook 110:** Intelligent loops:\n",
    "```\n",
    "1. REASON: \"User wants firewall version, I should use get_firewall_version tool\"\n",
    "2. ACT: Call get_firewall_version(\"fw-prod-01\")\n",
    "3. OBSERVE: \"Version is 10.1.0\"\n",
    "4. REASON: \"Now I can answer the user's question\"\n",
    "5. RESPOND: \"Your firewall fw-prod-01 is running PAN-OS 10.1.0\"\n",
    "```\n",
    "\n",
    "### 4. What You'll Build Next\n",
    "\n",
    "In notebook 110, you'll create a PAN-OS agent that can:\n",
    "\n",
    "‚úÖ **Remember conversations** (with automatic reducers)  \n",
    "‚úÖ **Call real tools** (check versions, create objects, modify configs)  \n",
    "‚úÖ **Reason about actions** (ReAct pattern)  \n",
    "‚úÖ **Handle multi-step tasks** (plan ‚Üí act ‚Üí observe ‚Üí respond)  \n",
    "‚úÖ **No manual sync required** (reducers handle everything)  \n",
    "\n",
    "### Key Differences Summary\n",
    "\n",
    "| Feature | Notebook 109 (Manual) | Notebook 110 (Automatic) |\n",
    "|---------|----------------------|--------------------------|\n",
    "| **Memory** | Manual sync required | Automatic with reducers |\n",
    "| **State Merging** | `history = result[\"messages\"]` | `Annotated[list, add_messages]` |\n",
    "| **Actions** | Can only talk | Can use tools |\n",
    "| **Pattern** | Linear (START‚ÜíEND) | ReAct (Reason‚ÜíAct‚ÜíObserve) |\n",
    "| **Code** | Lots of boilerplate | Clean and concise |\n",
    "| **Error Risk** | High (forget sync) | Low (automatic) |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ Conversation memory requires tracking BOTH human and AI messages  \n",
    "‚úÖ Manual memory means appending AI responses to state  \n",
    "‚úÖ History synchronization is critical across invocations  \n",
    "‚úÖ Forgetting to sync breaks memory completely  \n",
    "‚úÖ Real-world wizards (NAT, address objects) show practical value  \n",
    "‚úÖ Persistence and cost management are real production concerns  \n",
    "‚úÖ LangGraph provides better solutions (reducers + tools)  \n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "**Want more practice?** Try these exercises:\n",
    "\n",
    "1. **Implement summarization trimming**: Instead of window trimming, use an LLM to summarize old messages\n",
    "2. **Add conversation export**: Create a function to export conversations to markdown format\n",
    "3. **Build a security policy wizard**: Multi-turn wizard for creating security rules\n",
    "4. **Implement conversation branching**: Save/load different conversation threads\n",
    "5. **Add conversation analytics**: Track average message length, turn count, common topics\n",
    "6. **Create conversation search**: Find past conversations by keyword\n",
    "\n",
    "### Ready for Notebook 110?\n",
    "\n",
    "You now have a **deep understanding** of how conversation memory works at the fundamental level. This knowledge will make notebook 110's automatic features much more meaningful - you'll understand what they're doing under the hood.\n",
    "\n",
    "**In notebook 110**, you'll learn:\n",
    "- ‚ú® The `add_messages` reducer for automatic message merging\n",
    "- ‚ú® Creating and using tools with `@tool` decorator\n",
    "- ‚ú® The ReAct (Reasoning and Acting) agent pattern\n",
    "- ‚ú® Building agents that can take real actions with SCM\n",
    "- ‚ú® No more manual synchronization - ever!\n",
    "\n",
    "**Remember:** You don't need to memorize all of this! The important thing is understanding that:\n",
    "- Conversation memory requires tracking message history\n",
    "- This can be done manually (tedious but instructive)\n",
    "- LangGraph's reducers automate the tedious parts\n",
    "- Understanding manual approach helps you debug and customize\n",
    "\n",
    "Great work! You're now ready for ReAct agents with automatic memory and tools! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to:** [110 LangGraph: ReAct Agents with Tools](110_react_agents_with_tools.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
